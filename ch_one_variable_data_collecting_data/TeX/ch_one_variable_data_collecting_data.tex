\begin{chapterpage}{Exploring one-variable data and collecting data}
  \chaptertitle{Exploring one-variable data and collecting data}
  \label{onevariabledata}
  \label{ch_one_variable_data}
  \label{collectingdata}
  \label{ch_collecting_data}
\label{ch_one_variable_data_collecting_data}
  \chaptersection{dataBasics}
  \chaptersection{categoricalData}
  \chaptersection{numericalData}
\chaptersection{numericalSummariesAndBoxPlots}
  \chaptersection{overviewOfDataCollectionPrinciples}
  \chaptersection{section_obs_data_sampling}
  \chaptersection{experimentsSection}
\end{chapterpage}
\renewcommand{\chapterfolder}{ch_one_variable_data_collecting_data}


\chapterintro{Scientists seek to answer questions
  using rigorous methods and careful observations.
  These observations -- collected from the likes of field notes,
  surveys, and experiments -- form the backbone of a statistical
  investigation and are called \term{data}.
  Statistics is the study of how best to collect, analyze,
  and draw conclusions from data.  \\

\noindent In this chapter, we begin by introducing the four-step statistical process and some important data basics that will lay the groundwork for our statistical investigations.  We look at ways to verbally, graphically, and numerically summarize data once it is collected.  Then we investigate techniques for collecting
data and common sources of bias that arise during data collection.  After finishing this chapter, you will have the tools for identifying strengths
and weaknesses in data-based conclusions, tools that are essential to be an informed
citizen and a savvy consumer of information.

}


\section{Introduction to data}
\label{dataBasics}

\sectionintro{
\noindent%
You collect data on dozens of questions from all of the students at your school.
How would you organize all of this data?
Effective presentation and description of data is a first step in most analyses.
This section introduces one structure for organizing data as well as some terminology that will be used throughout this book.
We also introduce loan data from Lending Club which will be used extensively in this chapter.

%%
\subsection*{Learning objectives}
\begin{enumerate}
\setlength{\itemsep}{0mm}
\item Identify components within a statistical study and distinguish between a population and a sample.

\item Determine an investigative question within a statistical study.

\item Identify observational units, variables, parameters, and statistics from a statistical study or data set.

\item Identify variables as categorical or numerical and numerical variables as discrete or continuous.

\end{enumerate}
}

\subsection{Why do we collect data?}

Researchers from a wide array of fields have questions or problems that require the collection and analysis of data.     Let's consider three examples.
\begin{itemize}
\setlength{\itemsep}{0mm}
\item Climate scientists: how will the global temperature change over the next 100 years?
\item Psychology: can a simple reminder about saving money cause students to spend less?
\item Political science: what fraction of adults in the United States approve of the job Congress is doing?
\end{itemize}

\noindent What questions from current events or from your own life can you think of that could be answered by collecting and analyzing data?  %While the questions that can be posed are incredibly diverse, many of these investigations can be addressed with a small number of data collection techniques, analytic tools, and fundamental concepts in statistical inference.

Before diving into statistical terminology and methods, it is helpful to put statistics in the context of a general process of investigation:
\begin{enumerate}
\setlength{\itemsep}{0mm}
\item Identify a statistical question or problem.
\item Collect relevant data on the topic.
\item Analyze the data.
\item Form a conclusion.
%\item Make decisions based on the conclusion.
\end{enumerate}
Questions whose answers are already known or questions that cannot be address with the collection of data are not \emph{statistical questions}.  Statistics as a subject has three primary components: How best can we collect data? How should it be analyzed? And what can we infer from the analysis?

We should think of this process as a feedback loop; one conclusion may prompt a follow-up question -- or possibly many follow-up questions.  When posing a statistical question, it is important that it be well-formed so that a clear plan for data collection can be made.  Moreover, the research question should not change based on the data analysis or results.  Avoiding bias and maintaining integrity in data collection and analysis is fundamental to the statistical process.

\subsection{Populations and samples}

Consider the following research questions:
\begin{enumerate}
\setlength{\itemsep}{0mm}
\item What is the average mercury content in swordfish in the Atlantic Ocean?
\item\label{voteForPresident}  What proportion of eligible voters plan to actually vote in the next United States presidential election?
\item\label{identifyPopulationOfStentStudy}  Does a new drug reduce the number of deaths in patients with severe heart disease?
\end{enumerate}

Each research question refers to a target \term{population}, or population of interest.  In the first question, the target population is all swordfish in the Atlantic ocean, and each fish represents an individual. Often times, it is too expensive to collect data for every individual in a population. Instead, a sample is taken. A \term{sample} represents a subset of the individuals and is often a small fraction of the population. For instance, 60 swordfish (or some other number) in the population might be selected, and this sample data may be used to provide an estimate of the population average in order to answer the research question.  We use a sample size \textit{n} from the entire population size \textit{N}.  Here, \textit{n} is 60 and the total number of swordfish in the Atlantic ocean (\textit{N}) is unknown.  When data are collected from a sample to answer an investigative question about a larger population, we call that investigation a \term{statistical study}. 

\begin{exercisewrap}
\begin{nexercise} \label{identifyingThePopulationForTwoQuestionsInPopAndSampSubsection}
For the second and third research questions above, identify the population of interest.  For these Guided Practice questions, you can check your answer
in the footnote.\footnotemark
\end{nexercise}
\end{exercisewrap}
\footnotetext{(\ref{voteForPresident}) The population of interest is eligible voters in the United States. (\ref{identifyPopulationOfStentStudy}) The population of interest includes all people with severe heart disease.}




\subsection{Observational units, variables, parameters, and statistics}



We collect a sample of data to better understand the characteristics of a population. A \term{variable} is a characteristic we measure for each individual of the population.  We call each individual of the population an \term{observational unit} or a \term{case}.  We may be interested in estimating a mean, median, proportion, or some other summary of a population. These population values are called \termsub{parameters}{parameter}.  More specifically, a parameter is a numerical attribute or summary of a variable of interest for a population.  Such summary values we calculate from a particular sample are called \termsub{statistics}{statistic}.  In general, we use a calculated statistic to estimate an unknown parameter.  



\begin{examplewrap}
\begin{nexample}{Earlier we asked the question: what is the average mercury content in swordfish in the Atlantic Ocean? Identify the variable to be measured, the observational unit, the parameter of interest, and the corresponding statistic.}The variable is the level of mercury content in swordfish in the Atlantic Ocean. It will be measured for each individual swordfish, which is the observational unit. The parameter of interest is the average mercury content in \emph{all} swordfish in the Atlantic Ocean, while the statistic will be the average mercury content in the swordfish in our sample.
\end{nexample}
\end{examplewrap}

\D{\newpage}

\begin{exercisewrap}
\begin{nexercise}For the second question regarding the proportion of eligible voters that plan to vote in the next United States presidential election, identify the observational unit, the variable to be measured, the parameter of interest, and the statistic.\footnotemark
\end{nexercise}
\end{exercisewrap}
\footnotetext{The observational unit is an individual eligible voter.  The variable, or characteristic that we record on each eligible voter, is whether or not they plan to vote in the next US presidential election. The parameter of interest is the proportion of all eligible voters that plan to vote in the next US presidential election, and the statistic will be the proportion in our sample that plan to vote in the next US presidential election.  }

\begin{examplewrap}
\begin{nexample}{Based on the definitions of statistic and parameter, which is a variable and which is a fixed number?}
A statistic can vary from sample to sample, so it is a variable.  A parameter is a numerical attribute or summary of a population, so at any one point in time, it is a fixed quantity (which is often unknown).
\end{nexample}
\end{examplewrap}

\subsection{Data matrices}

\index{data!loan50|(}

The information that we collect on each individual is called a \term{datum} (singular of \term{data}).  A collection of data is called a \term{data set}.  Figure~\ref{loan50DF} displays rows 1, 2, 3, and 50 of a data set
for 50 randomly sampled loans offered through Lending Club,
which is a peer-to-peer lending company.  These observations will be referred to as the
\data{loan50} data set.  


Each row in the table represents a single loan.  Each row corresponds to an observational unit.  An \term{observational unit} or \term{case} is an item or individual from which data is collected.
The columns represent characteristics,
called variables, for each of the loans.  A \term{variable} is a characteristic that may change from one observational unit to another.  For example, the first row represents a loan of \$7,500 with an interest rate of 7.34\%, where the borrower is based in Maryland (MD) and has an income of \$70,000.

\begin{exercisewrap}
\begin{nexercise}
What is the grade of the first loan in Figure~\ref{loan50DF}?
And what is the home ownership status of the borrower
for that first loan?
\footnotemark{}
\end{nexercise}
\end{exercisewrap}
\footnotetext{The loan's grade is A,
  and the borrower rents their residence.}

In practice, it is especially important to ask clarifying
questions to ensure important aspects of the data are understood.
For instance, it is always important to be sure we know what
each variable means and the units of measurement.
Descriptions of the \data{loan50} variables are given
in Figure~\ref{loan50Variables}.

\begin{figure}[h]
\centering
{\small
\begin{tabular}{ccc ccc cc} %c}
  \hline
   & \var{loan\us{}amount}
   & \var{interest\us{}rate}
   & \var{term} & \var{grade} & \var{state}
   & \var{total\us{}income}
   & \var{homeownership} \\
  \hline
  1 & 7500 & 7.34 & 36 & A & MD & 70000 & rent \\
  2 & 25000 & 9.43 & 60 & B & OH & 254000 & mortgage \\
  3 & 14500 & 6.08 & 36 & A & MO & 80000 & mortgage \\
  $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$
      & $\vdots$ & $\vdots$ \\
  50 & 3000 & 7.96 & 36 & A & CA & 34000 & rent \\
   \hline
\end{tabular}
}
\caption{Four rows from the \data{loan50} data matrix.}
\label{loan50DF}
\end{figure}
% Dropped: state, verified_income
% library(openintro); vars <- c("loan_amount", "interest_rate", "term", "grade", "total_income", "home_ownership", "loan_status"); library(xtable); data(loan50); loan50[c(1,2,3,50), vars]; xtable(loan50[c(1,2,3,50), vars])

\D{\newpage}

\begin{figure}[h]
\centering\small
\begin{tabular}{lp{10.5cm}}
\hline
{\bf variable} & {\bf description} \\
\hline
\var{loan\us{}amount} & Amount of the loan received,
    in US dollars.  \\
\var{interest\us{}rate} & Interest rate on the loan,
    in an annual percentage.  \\
\var{term} & The length of the loan, which is always set
    as a whole number of months. \\
\var{grade} & Loan grade, which takes values A through G
    and represents the quality of the loan and its likelihood
    of being repaid.  \\
\var{state} & US state where the borrower resides. \\
\var{total\us{}income} & Borrower's total income,
    including any second income, in US dollars.   \\
\var{homeownership} & Indicates whether the
    person owns, owns but has a mortgage, or rents.  \\
%\var{verified\us{}income} & Indicates whether the
%    income is verified, its source is verified but not the amount,
%    or it is not verified.   \\
\hline
\end{tabular}
\caption{Variables and their descriptions for the \data{loan50} data set.}
\label{loan50Variables}
\end{figure}

\index{data!loan50|)}

The data in Figure~\ref{loan50DF} represent a \term{data matrix},
which is a convenient and common way to organize data,
especially if collecting data in a spreadsheet.
Each row of a data matrix corresponds to a unique case
(observational unit),
and each column corresponds to a variable.
%A data matrix for the stroke study introduced in
%Section~\ref{basicExampleOfStentsAndStrokes} is shown
%in Figure~\vref{stentStudyResultsDF}, where the cases were
%patients and three variables were recorded for each
%patient.

When recording data, use a data matrix unless you have
a very good reason to use a different structure.
This structure allows new cases to be added as rows
or new variables as new columns.

\begin{exercisewrap}
\begin{nexercise}
The grades for assignments, quizzes, and exams in a course are
often recorded in a gradebook that takes the form of a data matrix.
How might you organize grade data using a data
matrix?\footnotemark
\end{nexercise}
\end{exercisewrap}

%\index{data!county|(}
%
%\begin{exercisewrap}
%\begin{nexercise}\label{desc_county_as_data_matrix}%
%Let's consider data for 3,142 counties in the United States,
%which includes each county's name,
%the state in which it is located, its population in 2017,
%how its population changed from 2010 to 2017,
%poverty rate,
%and six additional characteristics.
%How might these data be organized in
%a data matrix?\footnotemark
%\end{nexercise}
%\end{exercisewrap}
%\addtocounter{footnote}{-1}
%\footnotetext{We can have each student represented by a row,
%  and then add a column for each assignment, quiz, or exam.
%  Under this setup, it is easy to review a single line to understand
%  a student's grade history.
%  There should also be columns to include student information,
%  such as one column to list student names.}
%\addtocounter{footnote}{1}
%\footnotetext{Each county may be viewed as a case,
%  and there are eleven pieces of information recorded for
%  each case.
%  A table with 3,142 rows and 11 columns could hold these data,
%  where each row represents a county and each column represents
%  a particular piece of information.}
%
%\noindent The data described in Guided
%Practice~\ref{desc_county_as_data_matrix} represents the
%\data{county} data set, which is shown as a data matrix
%in Figure~\ref{countyDF}.
%These data come from the US Census, with much of
%the data coming from the US Census Bureau's American
%Community Survey (ACS).
%Unlike the Decennial Census, which takes place every 10 years and attempts to collect basic demographic data from every resident of the US, the ACS is an ongoing survey that is sent to approximately 3.5 million households per year.
%As stated by the ACS website, these data help communities ``plan for hospitals and schools, support school lunch programs, improve emergency services, build bridges, and inform businesses looking to add jobs and expand to new markets, and more."\footnote{\oiRedirect{textbook-acs}{\url{https://www.census.gov/programs-surveys/acs/about.html}}}
%A small subset of the variables from the ACS are summarized in Figure~\ref{countyVariables}.
%
%\D{\newpage}
%
%\begin{landscape}
%\begin{figure}
%\centering\small
%\begin{tabular}{ccc ccc ccc ccc}
%  \hline
% & \var{name} & \var{state} & \var{pop} & \var{pop\us{}change} & \var{poverty} & \var{homeownership} & \var{multi\us{}unit} & \var{unemp\us{}rate} & \var{metro} & \var{median\us{}edu} & \var{median\us{}hh\us{}income} \\ 
%  \hline
%1 & Autauga  & Alabama &  55504 &  1.48 & 13.7 & 77.5 &  7.2 & 3.86 & yes & some\us{}college & 55317 \\ 
%  2 & Baldwin  & Alabama & 212628 &  9.19 & 11.8 & 76.7 & 22.6 & 3.99 & yes & some\us{}college & 52562 \\ 
%  3 & Barbour  & Alabama &  25270 & -6.22 & 27.2 & 68.0 & 11.1 & 5.90 & no  & hs\us{}diploma   & 33368 \\ 
%  4 & Bibb     & Alabama &  22668 &  0.73 & 15.2 & 82.9 &  6.6 & 4.39 & yes & hs\us{}diploma   & 43404 \\ 
%  5 & Blount   & Alabama &  58013 &  0.68 & 15.6 & 82.0 &  3.7 & 4.02 & yes & hs\us{}diploma   & 47412 \\ 
%  6 & Bullock  & Alabama &  10309 & -2.28 & 28.5 & 76.9 &  9.9 & 4.93 & no  & hs\us{}diploma   & 29655 \\ 
%  7 & Butler   & Alabama &  19825 & -2.69 & 24.4 & 69.0 & 13.7 & 5.49 & no  & hs\us{}diploma   & 36326 \\ 
%  8 & Calhoun  & Alabama & 114728 & -1.51 & 18.6 & 70.7 & 14.3 & 4.93 & yes & some\us{}college & 43686 \\ 
%  9 & Chambers & Alabama &  33713 & -1.20 & 18.8 & 71.4 &  8.7 & 4.08 & no  & hs\us{}diploma   & 37342 \\ 
%  10 & Cherokee & Alabama &  25857 & -0.60 & 16.1 & 77.5 &  4.3 & 4.05 & no  & hs\us{}diploma   & 40041 \\ 
%  $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ \\
%  3142 & Weston   & Wyoming &   6927 & -2.93 & 14.4 & 77.9 &  6.5 & 3.98 & no  & some\us{}college & 59605 \\  
%   \hline
%\end{tabular}
%\caption{Eleven rows from the \data{county} data set.}
%\label{countyDF}
% library(openintro); data(county); county$name <- gsub(" County$", "", county$name); county$pop <- county$pop2017; county$unemp_rate = county$unemployment_rate; these <- c("name", "state", "pop", "pop_change", "poverty", "homeownership", "multi_unit", "unemp_rate", "metro", "median_edu", "median_hh_income"); county <- county[, these]; library(xtable); xtable(as.data.frame(lapply(rbind.data.frame(head(county, 10), tail(county, 1)), function(x) { format(x) })))
%\end{figure}
%
%\begin{figure}
%\centering\small
%\begin{tabular}{lp{11cm}}
%\hline
%{\bf variable} & {\bf description} \\
%\hline
%\var{name} &
%    County name. \\
%\var{state} &
%    State where the county resides,
%    or the District of Columbia. \\
%\var{pop} &
%    Population in 2017. \\
%\var{pop\us{}change} &
%    Percent change in the population from 2010 to 2017.
%    For example, the value \resp{1.48} in the first row
%    means the population for this county
%    increased by 1.48\% from 2010 to 2017. \\
%\var{poverty} &
%    Percent of the population in poverty. \\
%\var{homeownership}  &
%    Percent of the population that lives in their own home
%    or lives with the owner, e.g. children living with parents
%    who own the home. \\
%\var{multi\us{}unit}  &
%    Percent of living units that are in multi-unit structures,
%    e.g. apartments. \\
%\var{unemp\us{}rate} &
%    Unemployment rate as a percent. \\
%\var{metro} &
%    Whether the county contains a metropolitan area. \\
%\var{median\us{}edu} & Median education level, which
%    can take a value among
%    \resp{below\us{}hs},
%    \resp{hs\us{}diploma},
%    \resp{some\us{}college},
%    and \resp{bachelors}. \\
%\var{median\us{}hh\us{}income} &
%    Median household income for the county, where a household's
%    income equals the total income of its occupants who are
%    15~years or older. \\
%\var{per\us{}capita\us{}income} &
%    Per capita (per person) income for the county. \\
%\hline
%\end{tabular}
%\centering
%\caption{Variables and their descriptions for the \data{county} data set.}
%\label{countyVariables}
%\end{figure}
%\end{landscape}

\subsection{Types of variables}
\label{variableTypes}

Examine the \var{interest\us{}rate}, \var{term}, \var{state},
and \var{grade} variables in the \data{loan50}
data set. Each of these variables is inherently different from the
other three, yet some share certain characteristics.

First consider \var{interest\us{}rate},
which is said to be a \term{numerical} variable because
it can take a wide range of numerical values,
and it is sensible to add, subtract, or take averages
with those values.
On the other hand, we would not classify a variable
reporting telephone area codes as numerical because the
average, sum, and difference of area codes doesn't have
any clear meaning.

The \var{term} variable is also numerical, although it seems
to be a little different than \var{interest\us{}rate}.
This variable represents number of months and can only take whole
non-negative numbers such as 36 and 60.
For~this reason, the \var{term} variable is said to be
\term{discrete} because it can only take numerical values
with jumps.
On the other hand, the \var{interest\us{}rate} variable is said
to be \term{continuous}.

The variable \var{state} can take up to 51 values after
accounting for Washington, DC: \resp{AL}, \resp{AK}, ...,
and \resp{WY}.
Because the responses themselves are categories,
\var{state} is called a \term{categorical} variable,
and the possible values are called the variable's \term{levels}.

Finally, consider the \var{grade} variable, which represents the quality of the loan and takes on values A through G.
This variable seems to be a hybrid: it is a categorical variable
but the levels have a natural ordering.
A variable with these properties is called an \term{ordinal}
variable, while a regular categorical variable without this
type of special ordering is called a \term{nominal} variable.
To simplify analyses, any ordinal variable in this book will
be treated as a nominal (unordered) categorical variable.

\begin{figure}[h]
  \centering
   \Figure
    [Breakdown of variables into their respective types, showing "all variables" breaking down into "numeric" and "categorical". Then "numeric" is divided into "continuous" and "discrete", and "categorical" is broken down into "nominal (unordered categorical)" and "ordinal (ordered categorical)'' variables.]
    {0.57}{variables}
  \caption{Breakdown of variables into their respective types.}
  \label{variables}
\end{figure}

\begin{examplewrap}
\begin{nexample}{Data were collected about students
    in a statistics course.
    Three variables were recorded for each student:
    number of pets, student height, and whether
    the student had previously taken a statistics course.
    Classify each of the variables as continuous numerical,
    discrete numerical, or categorical.}
  The number of pets and student height represent
  numerical variables.
  Because the number of pets is a count, it is discrete.
  Height varies continuously, so it is a continuous numerical
  variable.
  The last variable classifies students into two categories
  -- those who have and those who have not taken a statistics
  course -- which makes this variable categorical.
\end{nexample}
\end{examplewrap}

\begin{exercisewrap}
\begin{nexercise}\index{data!stroke}%
An experiment is evaluating the effectiveness of a new drug
in treating migraines.
A \var{group} variable is used to indicate the experiment group
for each patient: treatment or control.
The \mbox{\var{num\us{}migraines}} variable represents the number
of migraines the patient experienced during a 3-month period.
\mbox{Classify} each variable as either numerical or
categorical.\footnotemark
\end{nexercise}
\end{exercisewrap}
\footnotetext{The
  \var{group} variable can take just one of two group names,
  making it categorical.
  The \var{num\us{}migraines} variable describes
  a count of the number of migraines, which is an outcome where
  basic arithmetic is sensible, which means this is a numerical
  outcome; more specifically, because it represents a count,
  \var{num\us{}migraines} is a discrete numerical variable.}


\D{\newpage}


%%
\subsection*{Section summary}
\begin{itemize}
 
\item Statistics can be understood as an investigative process that involves asking a valid statistical research question, collecting data, analyzing the data, and forming a conclusion.

\item A \termni{population} is the entire group of interest.  The population size is represented by $N$.

\item A \termni{sample} is the subset of the population that a researcher collects data on.  The number of item in the sample, called the sample size, is represented by $n$.

\item When data are collected from a sample to answer an investigative question about a population, we call this a \termni{statistical study}.  Statistical studies are necessary when it is too difficult or expensive to collect data from every individual in the population.

\item A \termni{datum} (singular form of data) is a piece of information about an individual.  A collection of data is called a \termni{data set}.

\item An investigative question for a statistical study should have a defined purpose, should not change based on the data analysis or results, and should be posed so that the required data can be collected and analyzed.

\item Researchers often summarize data in a table, where the rows correspond to individuals, also called \termni{observational units} or \termni{cases}, and the columns correspond to the \termni{variables}, the values of which are recorded for each individual. 

\item A \termni{parameter} is a numerical attribute or summary of a variable of interest for the entire population.  Researchers take a sample from the population to estimate this unknown quantity.  The estimate calculated from the sample is called the \termni{statistic}. A statistic can vary from sample to sample, while a parameter is a fixed number.

\item Variables can be \termni{numerical} (measured on a numerical scale) or \termni{categorical} (taking on levels, such as low/medium/high).  Numerical variables can be \termni{continuous}, where all values within a range are possible, or \termni{discrete}, where only specific  values, usually integer values, are possible.


\end{itemize}



%%%section exercises
{\input{ch_one_variable_data_collecting_data/TeX/introduction_to_data.tex}}


%______________________________________________
\section[Representing categorical data]{Representing categorical data }
\label{categoricalData}


\sectionintro{


%% fix
\noindent%
How do we visualize and summarize categorical data?
In this section, we will introduce tables and other basic tools for categorical data that are used throughout this book.


% library(openintro); data(email); dim(email)

%% fix
\subsection*{Learning objectives}
\begin{enumerate}
\setlength{\itemsep}{0mm}


\item Construct and interpret tabular representations (one-way tables) for a categorical variable.  
\item Describe and summarize a categorical variable with counts, proportions, ratios, and percents.  
\item Interpret graphical representations of a categorical variable.
\item Justify a claim using tabular and graphical representations of a categorical variable.
\item Use tabular and graphical representations to compare two or more data sets in terms of the same categorical variable.  


\end{enumerate}
}


%%
\subsection{Tabular representations of a categorical variable}
% library(openintro); dim(loans_full_schema)

\newcommand{\loanapphomeAA}{3496}
\newcommand{\loanapphomeAB}{3839}
\newcommand{\loanapphomeAC}{1170}
\newcommand{\loanapphomeAD}{8505}
\newcommand{\loanapphomeBA}{362}
\newcommand{\loanapphomeBB}{950}
\newcommand{\loanapphomeBC}{183}
\newcommand{\loanapphomeBD}{1495}
\newcommand{\loanapphomeDA}{3858}
\newcommand{\loanapphomeDAPt}{0.3858} % Overall frequency
\newcommand{\loanapphomeDB}{4789}
\newcommand{\loanapphomeDC}{1353}
\newcommand{\loanapphomeDD}{10000}
\newcommand{\loanapphomeDAp}{0.3858}
\newcommand{\loanapphomeDBp}{0.4789}
\newcommand{\loanapphomeDCp}{0.1353}
\newcommand{\loanapphomeN}{\loanapphomeDD{}}
\index{data!loans\_full\_schema|(}



Recall the \data{loan50} data set we encountered in the previous section.  This data set represents a sample from a much larger
loan data set called \data{loans\us{}full\us{}schema}.  This larger data set contains information on 10,000 loans made
through Lending Club.  One of the variables in the \data{loans\us{}full\us{}schema} data set is \var{homeownership}.  This is a categorical variable with three levels: rent, mortgage, own.  
Consider Figure~\ref{loan_homeownership_totals}
for the \var{homeownership} variable, where each count in the table represents the number of times a particular variable outcome occurred in the data set. For example, the value  \loanapphomeDA{} corresponds to the number of loans in the data set where the borrower rents their home. A table like this that summarizes counts for each value type of a categorical variable is called a \term{frequency table}.



\begin{figure}[htb]
\centering
\begin{tabular}{lc}
  \hline
  \var{homeownership} & Count \\
  \hline
  mortgage & \loanapphomeDB{} \\
  own & \loanapphomeDC{} \\
  rent & \loanapphomeDA{} \\
  \hline
  Total & \loanapphomeDD{} \\ 
  \hline
\end{tabular}
\caption{A table summarizing the frequencies of each
    value for the \var{homeownership} variable.}
\label{loan_homeownership_totals}
\end{figure}

\D{\newpage}

Sometimes it is more helpful to record the proportion of times a particular variable outcome occurred, as in Figure~\ref{loan_homeownership_totals_2}.  In this case, we can use a \term{relative frequency table}.  The total for a relative frequency table always adds to 1, because all cases are represented on the table.

\begin{figure}[htb]
\centering
\begin{tabular}{lc}
  \hline
  \var{homeownership} & Relative frequency \\
  \hline
  mortgage & \loanapphomeDBp{} \\
  own & \loanapphomeDCp{} \\
  rent & \loanapphomeDAp{} \\
  \hline
  Total & 1 \\ 
  \hline
\end{tabular}
\caption{A table summarizing the relative frequencies of each
    value for the \var{homeownership} variable.}
\label{loan_homeownership_totals_2}
\end{figure}

\subsection{Bar charts and pie charts}

A \term{bar~chart} (also called \termni{bar~plot} or \termni{bar~graph}) is a common way to display a single
categorical variable.  In a bar~chart, each bar represents a category of a categorical variable and the height of each bar corresponds to the frequency (count) or relative frequency (proportion) for that category.  The left panel of Figure~\ref{loan_homeownership_bar_plot} shows a frequency bar~chart for the \var{homeownership} variable.
In the right panel, the counts are converted into proportions,
showing the proportion of observations that are in each level
(e.g. $\loanapphomeDA{} / \loanapphomeDD{} = 0.3858$ for
  \resp{rent}).

\begin{figure}[bht]
  \centering
  \Figure[Two bar~charts, which are described as the left bar~chart and the right bar~chart. The left bar~chart has Homeownership on the horizontal axis and Frequency (count) on the Vertical axis. Each level of homeownership has its own "bar" (which looks like a tall rectangle resting on the horizontal axis) with a height corresponding the frequency of that bar in the data set. For example, the "Rent" bar extends from the horizontal axis up to a frequency of about 3900. The "Mortgage" bar extends from the horizontal axis up to about 4700, and the bar for "Own" extends up to at about 1300. Moving to the next plot, the right bar~chart, it looks very similar to the left bar~chart except that it reports the proportion of cases on the vertical axes instead of the frequency (count). The values in this bar~chart are: about 0.39 for Rent, about 0.47 for Mortgage, and about 0.13 for Own.]
{0.82}{loan_homeownership_bar_plot}
  \caption{Two bar~charts of \var{homeownership}.
      The left panel shows the counts, and the right panel
      shows the proportions in each group.}
  \label{loan_homeownership_bar_plot}
\end{figure}

\D{\newpage}

A pie chart is shown in Figure~\ref{loan_homeownership_pie_chart} representing the same information in the bar~charts in Figure~\ref{loan_homeownership_bar_plot}.  Each slice of a pie chart represents
a category of the categorical variable of interest. The area of each slice,
as a fraction of the total area, corresponds to the relative frequency of
observational units falling within each category. The sum of the slices’ areas
together will equal 1, or 100\% of the total area.  Pie charts can be useful for giving a high-level overview to show how a set of cases break down.  However, it is also difficult to decipher certain details in a pie chart. For example, it’s not immediately obvious that there are more loans where the borrower has a mortgage than rent when looking at the pie chart, while this detail is very obvious in the bar~chart.

\begin{figure}[h]
  \centering
 \Figure[A pie chart, which is a circle that has three lines drawn from the center of the circle to its edge, dividing the circle into "slices". The lower left slice is large, representing close to 50\% of the total circle, it is colored blue, and it is labeled "mortgage". The upper slice is also quite large, representing almost 40\% of the circle, is colored green, and it is labeled "rent". The lower right slice is much smaller, representing about 15\% of the circle, it is colored red, and it is labeled "own".]
{0.82}{loan_homeownership_pie_chart}
  \caption{A pie chart of \var{homeownership}. }
  \label{loan_homeownership_pie_chart}
\end{figure}

\index{data!loans\_full\_schema|)}

Pie charts can work well when the goal is to visualize a categorical variable with very few levels, especially if each level represents a simple fraction (e.g., one-half, one-quarter, etc.). However, they can be quite difficult to read when they are used to visualize a categorical variable with many levels. For example, the pie chart in Figure~\ref{loan_homeownership_pie_chart_2} and the bar~chart in Figure~\ref{loan_homeownership_bar_plot_2} both represent the distribution of loan grades (A through G). In this case, it is far easier to compare the counts of each loan grade using the bar~chart than the pie chart.

\begin{figure}[h]
  \centering
\begin{minipage}{.5\textwidth}
  \centering
  \Figure[..]{}{loan_homeownership_pie_chart_2}
  \caption{(a) Pie chart}
  \label{loan_homeownership_pie_chart_2}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
    \Figure[..]{}{loan_homeownership_bar_plot_2}
  \caption{(b) Bar chart}
  \label{loan_homeownership_bar_plot_2}
\end{minipage}%
\caption{A pie chart and bar~chart of loan grades.}
  \label{loan_homeownership_pie_chart_bar_chart}
\end{figure}

\D{\newpage}

\subsection{Comparing data sets in terms of a categorical variable}

We can use tabular and graphical summaries of a categorical variable to compare two or more groups or data sets.  Here, we compare \var{homeownership} for individual loans and for joint loans using relative frequency tables and bar~charts.  Because the number of individual loans and the number of joint loans are quite different, it makes more sense to compare proportions rather than numbers.

\begin{figure}[htb]
\centering
\begin{tabular}{lc | c lc}
  \hline
Homeownership for & Relative frequency & \quad & Homeownership for & Relative frequency \\
Individual loans & & & Joint loans & \\
  \hline
  mortgage & 0.4514 &\quad & mortgage & 0.6355\\
  own & 0.1376 & \quad & own & 0.1224 \\
  rent & 0.4110 & \quad & rent & 0.2421  \\
  \hline
\end{tabular}
\caption{Two tables summarizing the relative frequencies of each
    value for the \var{homeownership} variable.  The table on the left includes only individual loans and the table on the right includes only joint loans.  }
\label{loan_homeownership_totals_separated}
\end{figure}

\begin{figure}[h]
  \centering
 \Figures[Two bar charts summarizing the relative frequencies of each
    value for the \var{homeownership} variable.  The bar chart on the left includes only individual loans and the bar chart on the right includes only joint loans.]
{.85}{loan_homeownership_bar_plot}{loan_homeownership_bar_plot_comparison}
  \caption{Two bar charts summarizing the relative frequencies of each
    value for the \var{homeownership} variable.  The bar chart on the left includes only individual loans and the bar chart on the right includes only joint loans.  }
  \label{loan_homeownership_bar_plot_comparison}
\end{figure}

\begin{examplewrap}
\begin{nexample}{Using the bar~charts in Figure~\ref{loan_homeownership_bar_plot_comparison}, compare homeownership type for those with individual versus joint loans.  What differences can be observed?} A higher percent of joint loans than individual loans are for mortgage (a little over 60\% versus a little over 40\%), whereas a higher percent of individual loans than joint loans are for rent (about 40\% versus about 25\%).
\end{nexample}
\end{examplewrap}

\newpage

\subsection*{Section summary}
\begin{itemize}

\item Tabular representations of a single categorical variable include a (one-way) frequency table and a relative frequency table.  A \termni{frequency table} shows the \textbf{number} or \textbf{count} of observational units in each category of a categorical variable, while a \termni{relative frequency table shows} the \textbf{proportion} or \textbf{percent}.
 
\item Percentages, relative frequencies, and ratios all provide the same information as proportions.

\item \termni{Bar charts}, also called bar graphs, display frequencies (counts) or relative
frequencies (proportions) for the categories of a single categorical variable.
Each bar on a bar chart represents a category of the categorical variable of
interest. The height or length of each bar corresponds to the frequency.

\item \termni{Pie charts} are used to display frequencies (counts) or relative frequencies
(proportions) for categorical data. Each slice on a pie chart represents
a category of the categorical variable of interest. The area of each slice,
as a fraction of the total area, corresponds to the relative frequency of
observational units falling within each category. The sum of the slices’ areas
together will equal 1, or 100\% of the total area.

\item Pie charts can be more difficult to read and bar~charts are generally a better option.  

\item Frequency and relative frequency tables, bar charts, and pie charts can be used to compare two or more data sets in terms of the same categorical
variable.

\item Counts, relative frequencies, and graphical representations of categorical variables reveal information that can be used to justify claims about the variables in context.

\item Frequency and relative frequency tables, bar charts, and pie charts can be
used to compare two or more data sets in terms of the same categorical
variable.

\end{itemize}




%%%%%%%%%%%Section Exercises
{\input{ch_one_variable_data_collecting_data/TeX/representing_categorical_data.tex}}



%______________________________________________
\section[Representing numerical data with graphs]{Representing numerical data with graphs}
\label{numericalData}

\sectionintro{
\noindent%
How do we visualize and describe the distribution of household income for counties within the United States?  What shape would the distribution have?  What other features might be important to notice?
In this section, we will explore techniques for
summarizing numerical variables using the \data{loan50} data set, which was introduced in Section~\ref{dataBasics}.


\subsection*{Learning objectives}
\begin{enumerate}
\setlength{\itemsep}{0mm}

\item Understand what the term \textit{distribution} means and how to summarize it in a table or graph.

\item Create and interpret stem-and-leaf plots, dot plots, and histograms for visualizing the distribution of a numerical variable. 

\item Describe the shape of a distribution as approximately symmetric, right skewed, or left skewed, and identify a distribution as unimodal, bimodal, multimodal, or uniform.

\item Summarize a distribution with respect to center, spread, shape, gaps, clusters, and outliers.

\item Justify a claim using graphical representations of a quantitative variable.

\end{enumerate}
}



%%

\index{data!loan50|(}

% library(openintro); ind <- c(1:5, 50); d <- loan50$interest_rate; (m <- round(mean(d), 2)); d[ind]; (dev <- d - m)[ind]; (dev2 <- dev^2)[ind]; (s2 <- sum(dev2) / 49); (s <- sqrt(s2)); var(d); sd(d); median(d); IQR(d); quantile(d, c(0.25, 0.75))
\newcommand{\loanA}{10.90}
\newcommand{\loanB}{9.92}
\newcommand{\loanC}{26.30}
\newcommand{\loanD}{9.92}
\newcommand{\loanY}{9.43}
\newcommand{\loanZ}{6.08}
\newcommand{\loanAvg}{11.57}
\newcommand{\loanVar}{25.52}
\newcommand{\loanSD}{5.05}
\newcommand{\loanN}{50}
\newcommand{\loanMedianBelow}{9.93\%}
\newcommand{\loanMedianAbove}{9.93\%}
\newcommand{\loanMedian}{9.93\%}
\newcommand{\loanQA}{7.96}
\newcommand{\loanQC}{13.72}
\newcommand{\loanIQR}{5.76}
\newcommand{\loanAdev}{-0.67}
\newcommand{\loanBdev}{-1.65}
\newcommand{\loanCdev}{14.73}
\newcommand{\loanDdev}{-1.65}
\newcommand{\loanYdev}{-2.14}
\newcommand{\loanZdev}{-5.49}
\newcommand{\loanSmallestValue}{5.31}
\newcommand{\loanLargestValue}{26.30}



%%
\subsection{Stem-and-leaf plots and dot plots}
\label{dotPlot}

Here we revisit the \data{loan50} data set, which includes data on 50 randomly sampled loans offered through Lending Club, a peer-to-peer lending company, and we look at the variable \var{interest\us{}rate}. We would like to visualize and summarize the distribution of this numerical variable.  The term \term{distribution} refers to the values that a variable takes and the frequency of these values.  To simplify the \var{interest\us{}rate} data, we round the values to the nearest percent. For example, 10.9\% is recorded as 11.

\captionsetup{width=0.9\textwidth}

\begin{figure}[ht]
\centering
\begin{tabular}{rrrrrrrrrr}
  \hline
 11 & 10 & 26 & 10 & 9 & 10 & 17 & 6 & 8 & 13 \\
  17 & 5 & 7 & 5 & 8 & 25 & 18 & 10 & 8 & 19 \\
   14 & 20 & 9 & 10 & 11 & 5 & 7 & 15 & 12 & 13 \\
  11 & 9 & 10 & 7 & 18 & 17 & 8 & 6 & 7 & 7 \\
  13 & 16 & 11 & 10 & 9 & 10 & 21 & 11 & 9 & 6 \\
   \hline
\end{tabular}
\caption{The interest rate, in \%, for 50~loans from Lending Club.}
\end{figure}

\captionsetup{width=\mycaptionwidth}

Rather than look at the data as a list of numbers, which makes the distribution difficult to discern, we will organize it into a table called a \term{stem-and-leaf plot} shown in Figure~\ref{stemandleafloan50}. In a stem-and-leaf plot, each number is broken into two parts. The first part is called the \term{stem} and consists of the beginning digit(s). The second part is called the \term{leaf} and consists of the final digit(s). The stems are written in a column in ascending order, and the leaves that match up with those stems are written on the corresponding row, with the leaf values getting larger as they are farther from the stem. Figure~\ref{stemandleafloan50} shows a stem-and-leaf plot of the interest rate for 50 loans from Lending Club. The stem represents the tens place and the leaf represents the ones place. For example, \mbox{\texttt{0 $|$ 5}} corresponds to 5\% and \texttt{2 $|$ 6} corresponds to 26\%. When making a stem-and-leaf plot, remember to include a legend that describes what the stem and what the leaf represent. Without this, there is no way of knowing if 2 $|$ 6  represents 2.6, 26, 260, 2600, etc.

\begin{figure}[h]
\begin{verbatim}
                   0 | 55566677777888899999
                   1 | 00000000111112333456777889
                   2 | 0156
                 

                 Legend: 2 | 0 = 20%
\end{verbatim}
\caption{A stem-and-leaf plot of the interest rate in 50 loans.}
\label{stemandleafloan50}
\end{figure}

\D{\newpage}

\begin{exercisewrap}
\begin{nexercise}There are only four numbers on the bottom row. Why is this the case?\footnotemark
\end{nexercise}
\end{exercisewrap}
\footnotetext{There are only 4 loans with interest rates in the 20\% range.}

When there are too many numbers on one row or there are only a few stems, we \emph{split} each row into two halves, with the leaves from 0-4 on the first half and the leaves from 5-9 on the second half. The resulting graph is called a \termsub{split stem-and-leaf plot}{stem-and-leaf plot!split stem-and-leaf plot}. Figure~\ref{splitstemandleafloan50} shows the previous stem-and-leaf redone as a split stem-and-leaf.

\begin{figure}[h]
\begin{verbatim}
                          0 | 
                          0 | 55566677777888899999
                          1 | 000000001111123334
                          1 | 56777889
                          2 | 01
                          2 | 56
          

                        Legend: 2 | 5 = 25%
\end{verbatim}
\caption{A split stem-and-leaf.}
\label{splitstemandleafloan50}
\end{figure}


\begin{exercisewrap}
\begin{nexercise}
Rounding to the nearest percent, what is the lowest interest rate in the \data{loan50} data set? What is the largest?\footnotemark
\end{nexercise}
\end{exercisewrap}
\footnotetext{The lowest interest rate is 5\%, and the largest is 26\%. That is a big range!}

Another simple graph for univariate numerical data is a dot plot. A~\term{dot plot} represents each value of a numerical variable with a dot and can be oriented vertically or, more commonly, horizontally.  When oriented horizontally, each dot is placed above the horizontal axis at the location corresponding to the value of the observation, with nearly identical values stacked on top of each other.  The higher the stack of dots, the greater the number of occurrences there are at those values.  An example using the same data set, interest rate from 50 loans, is shown in Figure~\ref{loan_int_rate_dot_plot_stacked}.

\begin{figure}[h]
   \centering
  \Figures
    [A dot plot is shown for the variable ``Interest Rate (rounded to nearst percent)". There is a horizontal axis ranging from 5 to 26, and then several points are shown horizontally above the axis, scattered over the range. There is a higher density of points between 5 and 10, with a moderate density of points from 10 to 20, and then a few more observations above 20.]
{0.825}{loan_int_rate_dot_plot}{loan_int_rate_dot_plot_stacked}
   \caption{A dot plot of \var{interest\_\hspace{0.3mm}rate} for the \data{loan50} data set.}
   \label{loan_int_rate_dot_plot_stacked}
\end{figure}

%\begin{exercisewrap}
%\begin{nexercise}
%Imagine rotating the dot plot 90 degrees clockwise. What do you notice?\footnotemark
%\end{nexercise}
%\end{exercisewrap}
%\footnotetext{It has a similar shape as the stem-and-leaf plot! The values on the horizontal axis correspond to the stems and the number of dots in each interval correspond the number of leaves needed for each stem.}

Graphs such as this make it easy to observe important features of the data, such as frequency, the location of clusters, and the presence of gaps.

\begin{examplewrap}
\begin{nexample}{Based on the dot plot, which value has the highest frequency, and are there any gaps in interest rate for the \data{loan50} data set?}
The highest frequency is at 10\%.  There is a small gap between 21\% and 25\%.
\end{nexample}
\end{examplewrap}

\D{\newpage}

Additionally, we can easily identify any observations that appear to be unusually distant from the rest of the data. Unusually distant observations are called \termni{outliers}. Later in this chapter we will provide numerical rules of thumb for identifying outliers. For now, it is sufficient to identify them by observing gaps in the graph. In this case, the values 25\% and 26\% could be classified as outliers because they are numerically distant from most of the data.

\begin{onebox}{Outliers are extreme}
An \term{outlier} is an observation that is unusually small or large relative to the rest of the data.
\end{onebox}


\begin{onebox}{Why it is important to look for outliers}
Examination of data for possible outliers serves many useful purposes, including\vspace{-2mm}
\begin{enumerate}
\setlength{\itemsep}{0mm}
\item Identifying asymmetry in the distribution.
\item Identifying data collection or entry errors. For instance, we re-examined the loans purported to have an interest rate of 25\% and 26\% to ensure these value were accurate.
\item Providing insight into interesting properties of the data.\vspace{0.5mm}
\end{enumerate}\end{onebox}


\begin{exercisewrap}
\begin{nexercise}
Consider a data set that consists of the following numbers:  12, 12, 12, 12, 12, 13, 13, 14, 14, 15, 19. Which graph would better illustrate the data: a stem-and-leaf plot or a dot plot? Explain.\footnotemark
\end{nexercise}
\end{exercisewrap}
\footnotetext{Because all the values begin with 1, there would be only one stem (or two in a split stem-and-leaf). This would not provide a good sense of the distribution. For example, the gap between 15 and 19 would not be visually apparent. A dot plot would be better here.}

\D{\newpage}

%%
\subsection{Histograms}
\label{histogramsAndShape}

Stem-and-leaf plots and dot plots are ideal for displaying data from small samples because they show the exact values of the observations and how frequently they occur. However, they are impractical for larger samples. For larger samples, rather than showing the
value of each observation, we prefer to think of the value
as belonging to a \emph{bin}.
For example, in the \data{loan50} data set, we created
a table of counts for the number of loans with interest
rates between 5.0\% and 7.5\%, then the number of loans
with rates between 7.5\% and 10.0\%, and so on.
Observations that fall on the boundary of a bin
(e.g. 10.00\%) are allocated to the lower bin.
This tabulation is shown in Figure~\ref{binnedIntRateAmountTable}.
These binned counts are plotted as bars in
Figure~\ref{loan50IntRateHist} into what is called
a \term{histogram}, which resembles a more heavily binned
version of the stacked dot plot shown in
Figure~\ref{loan_int_rate_dot_plot_stacked}.

\begin{figure}[ht]
\centering\small
\begin{tabular}{l ccc ccc ccc}
  \hline
  Interest Rate &
      5.0\% - 7.5\% &
      7.5\% - 10.0\% &
      10.0\% - 12.5\% &
      12.5\% - 15.0\% &
      $\cdots$ &
      25.0\% - 27.5\% \\
  \hline
  Count & 11 & 15 & 8 & 4 & $\cdots$ & 1 \\
  \hline
\end{tabular}
\caption{Counts for the binned
    \var{interest\us{}rate} data.}
\label{binnedIntRateAmountTable}
\end{figure}
% library(openintro); library(xtable); d <- loan50$interest_rate; max(d); t1 <- table(cut(d, seq(5, 27.5, 2.5), right = TRUE)); t1; xtable(rbind(t1))

\begin{figure}[bth]
  \centering
  \Figure
    [A histogram with a horizontal axis of "Interest Rate" and a vertical axis showing the frequency of occurrence of different bins of interest rate. The first bin is from 5\%-7.5\% with a frequency (count) of 11 observations, 7.5\%-10\% has a frequency of 15, 10\%-12.5\% has 8, 12.5\%-15\% has 4, 15\%-17.5\% has 5, 17.5\%-20\% has 4, and then the 20\%-22.5\%, 22.5\%-25\%, and 25\%-27.5\% bins each have a frequency of 1.]
    {0.76}{loan50IntRateHist}
  \caption{A histogram of \var{interest\us{}rate}.
      This distribution is strongly skewed to the right.
      \index{skew!strong}}
  \label{loan50IntRateHist}
\end{figure}

\begin{exercisewrap}
\begin{nexercise}
What can you see in the dot plot and stem-and-leaf plot that you cannot see in the frequency histogram?\footnotemark
\end{nexercise}
\end{exercisewrap}
\footnotetext{Interest rates for individual loans.}


\begin{onebox}{Drawing histograms}
1. The variable should be placed on the horizontal axis. Before drawing the histogram, label both axes and draw a scale for each. \\[2mm]
2. Draw bars such that the width of the bar corresponds to the bin width and the height of the bar is the frequency of that bin.\end{onebox}

Histograms provide a view of the \term{data density}. Higher bars represent where the data are relatively more common. For instance, there are more loans with interest rates between 7.5\%-10\% than between 10\%-12.5\%. The bars make it easy to see how the density of the data changes relative to the number of loans.

\begin{examplewrap}
\begin{nexample}{How many loans had interest rates less than 10\%?}The height of the bars corresponds to frequency. There were 11 cases from 5 to less than 7.5 and 15 cases from 7.5 to less than 10, so there were $11+15=26$ loans that had interest rates less than 10\%.
\end{nexample}
\end{examplewrap}

\begin{examplewrap}
\begin{nexample}{Approximately how many loans had an interest rate less than 6\%?} Based just on this histogram, we cannot know the exact answer to this question. We only know that 11 loans had interest rates between 5 and 7.5 percent. If the number of loans is evenly distribution on this interval, then we can estimate that approximately 11/2.5~$\approx$ 4 loans fell in the range between 5 and 6 percent.
\end{nexample}
\end{examplewrap}

\begin{examplewrap}
\begin{nexample}{What \emph{percent} of the loans had interest rates greater than 10\%?}
From the first example, we know that 26 loans had interest rates less than 10\%. Because there are 50 loans in total, there must be 24 loans that had interest rates greater than 10\%. To find the percent, compute $24/50 = 0.48 = 48\%$.
\end{nexample}
\end{examplewrap}

Just as we constructed a frequency table and frequency histogram, we can construct a relative frequency table and relative frequency histogram where we represent the proportion rather than the number within each bin.

\begin{exercisewrap}
\begin{nexercise}
How will the appearance of a relative frequency histogram differ from that of the corresponding frequency histogram?\footnotemark
\end{nexercise}
\end{exercisewrap}
\footnotetext{Changing from frequency to relative frequency involves dividing all the frequencies by the same number, so only the vertical scale (the numbers on the y-axis) changes.  The appearance of the histogram remains the same.}

We see in Figure~\ref{loan50_freq_vs_rel_freq}, the frequency and relative frequency histograms differ only in the scale on the vertical axes.  Choose one or the other depending whether it is more important to be able to quickly read off the number or proportion in each bin.  When comparing distributions, it is generally better to use relative frequency histograms, especially when the number of observations represented in each graph are very different.


\begin{figure}[h]
  \centering
  \subfigure[]{
    \Figure[]
{0.45}
        {loan50IntRateHist}
    \label{loan50IntRateHist}
  }
  \subfigure[]{
    \Figures[]
{0.45}
        {loan50IntRateHist}{loan50IntRateHistRelativeFreq}
    \label{loan50IntRateHistRelativeFreq}
  }
  \caption{A comparison of a frequency histogram with its corresponding relative frequency histogram.}
  \label{loan50_freq_vs_rel_freq}
\end{figure}



\D{\newpage}

%%
\subsection{Describing shape}
\label{shape}

Histograms are especially convenient for describing the \term{shape} of the data distribution\label{shapeFirstDiscussed}. Figure~\ref{loan50IntRateHist} shows that more loans have a lower interest rate, while fewer loans have a large interest rate. When data trail off to the right in this way and have a longer right \hiddenterm{tail}, the shape is said to be \termsub{right skewed}{skew!right skewed}.\footnote{Other ways to describe data that are right skewed: \termni{skewed to the right}, \termni{skewed to the high end}, or \termni{skewed to the positive end}.} Data sets with the reverse characteristic -- a longer, thin tail to the left -- are said to be \termsub{left skewed}{skew!left skewed}.  Data sets that show roughly equal trailing off in both directions may be \termni{ approximately symmetric}.\index{skew!symmetric}

\begin{onebox}{Long tails to identify skew}
When data trail off in one direction, the distribution has a \termni{long tail}.  If a distribution has a longer left tail, it is left skewed. If a distribution has a longer right tail, it is right skewed.\end{onebox}

\begin{exercisewrap}
\begin{nexercise}
Take a look at the dot plot in Figure~\ref{loan_int_rate_dot_plot_stacked}. Can you see the skew in the data? Is it easier to see the skew in the frequency histogram, the dot plot, or the stem-and-leaf plot?\footnotemark
\end{nexercise}
\end{exercisewrap}
\footnotetext{The skew is visible in all three plots.}

\begin{exercisewrap}
\begin{nexercise}
Would you expect the distribution of number of pets per household to be right skewed, left skewed, or approximately symmetric?  Explain.\footnotemark
\end{nexercise}
\end{exercisewrap}
\footnotetext{We suspect most households would have 0, 1, or 2 pets but that a smaller number of households will have 3, 4, 5, or more pets.  Based on this, we would expect there to be greater density over the small numbers, suggesting the distribution will have a long right tail and be right skewed.}

In addition to looking at whether a distribution is skewed or symmetric, histograms, stem-and-leaf plots, and dot plots can be used to identify modes. A \term{mode} is represented by a prominent peak in the distribution.\footnote{Another definition of mode, which is not typically used in statistics, is the value with the most occurrences. It is common to have \emph{no} observations with the same value in a data set, which makes this other definition useless for many real data sets.} There is only one prominent peak in the histogram of \var{interest\_\hspace{0.3mm}rate}.

Figure~\ref{singleBiMultiModalPlots} shows histograms that have one, two, or three prominent peaks. Such distributions are called \termsub{unimodal}{modality!unimodal}, \termsub{bimodal}{modality!bimodal}, and \termsub{multimodal}{modality!multimodal}, respectively. Any distribution with more than 2 prominent peaks is called multimodal. Notice that in Figure~\ref{loan50IntRateHist} there was one prominent peak in the unimodal distribution with a second less prominent peak that was not counted, because it only differs from its neighboring bins by a few observations.

\begin{figure}[h]
   \centering
  \Figure
    [Three histograms are shown. The first histogram shows bins of width 2 between 0 to 18 (this is along the horizontal axis), and the frequencies are 3, 16, 16, 7, 11, 6, 4, 1, and 1. The second histogram, representing a different data set, shows bins of width 2 with values ranging from 0 to 20, where the bin counts in order are 2, 9, 5, 2, 2, 2, 2, 10, 19, and 9. The third histogram, representing yet another data set, shows bins of width 2 with values ranging from 0 to 22, where the bin counts in order are 10, 8, 4, 3, 1, 20, 15, 3, 15, 18, and 5.]
    {}{singleBiMultiModalPlots}
   \caption{Counting only prominent peaks, the distributions are (left to right) unimodal, bimodal, and multimodal.}
   \label{singleBiMultiModalPlots}
\end{figure}

When a distribution has no prominent peaks and when the frequency of each value is approximately the same, we call the distribution \term{uniform}, or uniformly distributed.

\begin{exercisewrap}
\begin{nexercise}
Height measurements of young students and adult teachers at a K-3 elementary school were taken. Would you anticipate the distribution of height for this data set to be uniform, unimodal, bimodal or multimodal?\footnotemark
\end{nexercise}
\end{exercisewrap}
\footnotetext{There might be two height groups visible in the data set: one of the students and one of the adults. That is, the data are probably bimodal.}

%\begin{onebox}{Looking for modes}
%Looking for modes isn't about finding a clear and correct answer about the number of modes in a distribution, which is why \emph{prominent} is not rigorously defined in this book. The important part of this examination is to better understand your data and how it might be structured.\end{onebox}


%%
\subsection{Summarizing distributions}
We can now summarize the shape of distributions of a numerical variable in terms of skew and modes.  We would also like to have some sense of the center and the variability, or spread, of a distribution.  We will quantify these terms in the next section.  For now let us develop an intuitive understanding of the terms center and spread by comparing histograms (a) and (b) in Figure~\ref{singleBiMultiModalPlots}.  

\begin{exercisewrap}
\begin{nexercise}
Compare histograms (a) and (b) in Figure~\ref{singleBiMultiModalPlots}.  Which distribution has a larger center?  Which has a larger spread?\end{nexercise}
\end{exercisewrap}
\footnotetext{Note that the horiztonal axis is not the same for graphs (a) and (b).  Visually, we would say that distribution (b) has a larger center.  If we very roughly estimate the center of distribution (a) at around 8, we can see that the center of distribution (b) is certainly higher than 8.  Also, distribution (b) has larger variability/spread as it has a larger range and less concentration in the middle.}

When summarizing the distribution of a numerical variable, it is helpful to comment on center, spread, and shape, as well as note the presence of cluster, gaps, or outliers.  For example, in graph (b) of Figure~\ref{singleBiMultiModalPlots}, we observe that there are two peaks and two clusters near the values of 4 and 18 and the distribution of the variable is somewhat left skewed.  There are no apparent gaps or outliers.    

The graphical summaries of this section and the numerical summaries of the next section fall into the realm of \term{descriptive statistics}.  Descriptive statistics is about describing or summarizing data; it does not attribute properties of the data to a larger population.  \termsub{Inferential statistics}{inferential statistics}, on the other hand, uses samples to generalize or to infer something about a larger population.  We will delve into inferential statistics in Chapter~\ref{ch_inference_for_props} and Chapter~\ref{ch_inference_for_means}.

\newpage

%%
\subsection*{Section summary}

\begin{itemize}
 
  \item When looking at a \termni{univariate} numerical display, researchers want to understand the distribution of the variable.  The term \termni{distribution} refers to the values that a variable takes and the frequency of those values.  

\item Two graphs that are useful for showing the distribution of a small number of observations are the stem-and-leaf plot and dot plot.  These graphs are ideal for displaying data from small samples because they show the exact values of the observations and how frequently they occur. For larger data sets it is common to use a histogram to display the distribution of a variable.  A histogram shows the frequency or relative frequency of values or intervals of values.

\item A \termni{histogram} places the observed values of the numerical variable into
ordered intervals, or bins, along the horizontal axis. Each bar represents an
interval or bin, and the height of each bar shows the frequency or relative
frequency of the observations within that interval. Altering the interval widths,
or bin widths, can change the appearance of the histogram. Alternatively, a
histogram can be constructed with bins on the vertical axis with bars appearing
horizontally.

\item A \termni{stem-and-leaf plot} splits each value of the numerical variable into two
parts: a ``stem” (the first digit or digits) and a ``leaf” (usually the single digit after
the stem digit or digits). Both stems and leaves are ordered from smallest to
largest.

\item A \termni{dot plot} represents each value of the numerical variable by a dot. Each dot
is placed above the horizontal or beside the vertical axis corresponding to the
value of that observation, with nearly identical values stacked on top of each
other.

\item Descriptions of the distribution of one numerical variable should include shape,
center, and variability (spread) as well as any unusual features such as outliers,
gaps, or clusters in context.

\item Distributions may be \termni{symmetric} or they may have a long tail.  If a distribution has a long left tail (with greater concentration over the higher numbers), it is \termni{left skewed}. If a distribution has a long right tail (with greater concentration over the smaller numbers), it is \termni{right skewed}.

\item Distributions may be \termni{unimodal}, \termni{bimodal}, or \termni{multimodal}, depending on the number of main peaks.  A distribution with no prominent peaks, where the frequency of each value is approximately the same is called \termni{uniform}.

\item An \term{outlier} is an observation that is unusually small or large relative to the rest of the data.  

\item A \termni{gap} is a region in a distribution between two values in which there are no observed data.  

\item \termni{Clusters} are concentrations of values usually separated by gaps.

\item Graphical representations of a quantitative variable may reveal information that can be used to justify claims about the variable in context.

\end{itemize}



%\Comment{maybe a separate section here on numerical summaries of (numerical) data}


%%%%%%%%%%%Section Exercises
{\input{ch_one_variable_data_collecting_data/TeX/representing_numerical_data_with_graphs.tex}}

%______________________________________________
\section[Numerical summaries and box plots]{Numerical summaries and box plots }
\label{numericalSummariesAndBoxPlots}

\sectionintro{
\noindent%
What are the different ways to measure the center of
a distribution, and why is there more than one way to
measure the center?
How do you know if a value is ``far" from the center?  What does it mean to be an outlier?  We will continue with the \data{loan50} data set and investigate multiple quantitative summarizes for numerical data.

%%
\subsection*{Learning objectives}
\begin{enumerate}
\setlength{\itemsep}{0mm}
\item Calculate, interpret, and compare the two measures of center (mean and median).

\item Calculate the five-number summary and interpret a percentile.

\item Calculate and interpret the three measures of spread (standard deviation, interquartile range, and range).

\item Describe the effect that changing units has on each of the summary quantities.

\item Identify and apply the two rules of thumb for calculating outliers.

\item Justify the selection of a summary statistic for describing quantitative data.

\item Construct and interpret a box plot, which provides a graphical representation of the five-number summary.

\item Understand how a distribution's shape affects the relationship between the mean and the median.

\item Compare the distribution of a numerical variable across groups, using multiple dot plots or histograms with the same scale, back-to-back stem-and-leaf plots, or parallel box plots.

\item Summarize and compare distributions of a numerical variable by commenting on center, spread, shape, and noticing the presence of cluster, gaps, and outliers. 

\item Justify a claim using multiple graphical representations or summary statistics of a quantitative variable.

\item Calculate $Z$-scores with population parameters and interpret them in context.

\item Compare $Z$-scores as measures of relative position for distributions.






\end{enumerate}
}


%%
\subsection{Measures of center}
\label{center}

In the previous section, we saw that modes can occur anywhere in a data set. Therefore, mode is not a measure of center. We understand the term \emph{center} intuitively, but there are multiple ways to formally define what is the center.  Here we will focus on the two most common: the mean and median.

The \term{mean}, often called the
\term{average}\index{mean!average}, is a common way
to measure the center of a distribution of data.
To compute the mean interest rate, we add up all the interest
rates and divide by the number of observations:
\begin{align*}
\bar{x}
    = \frac{\text{\loanA\%} + \text{\loanB\%} + \text{\loanC\%} +
        \cdots + \text{\loanZ\%}}{\loanN{}}
    = \loanAvg{}\%
% library(openintro); loan50$interest_rate[c(1:3, 50)]; mean(loan50$interest_rate)
\end{align*}
The sample mean is often labeled $\bar{x}$.
The letter $x$ is being used as a generic placeholder
for the variable of interest, \var{interest\us{}rate},
and the bar over the $x$ communicates we're looking at the
average interest rate, which for these 50 loans was \loanAvg{}\%.
It is useful to think of the mean as the balancing point
of the distribution, and it's shown as a triangle in Figure~\ref{loan_int_rate_dot_plot_stacked_triangle}.

\begin{figure}[h]
  \centering
  \Figures
    [A dot plot is shown for the variable "Interest Rate". There is a horizontal axis ranging from about 5\% to a bit over 25\%, and then several points are shown horizontally above the axis, scattered over the range. There is a higher density of points between 5\% to 11\%, with a moderate density of points from 12\% to about 20\%, and then a few more observations at about 22\%, 25\%, and 26\%. A red triangle is also shown at approximately 12\%.]
    {0.76}{loan_int_rate_dot_plot}{loan_int_rate_dot_plot_stacked_triangle}
  \caption{A dot plot of \var{interest\us{}rate}
      for the \data{loan50} data set.
      The distribution's mean is shown as a red triangle.}
  \label{loan_int_rate_dot_plot_stacked_triangle}
\end{figure}



\begin{onebox}{Mean}%
The sample mean can be computed as the sum of the
observed values divided by the number of observations:
\begin{align*}
\bar{x} = \frac{x_1 + x_2 + \cdots + x_n}{n}
\end{align*}
where $x_1$, $x_2$, $\dots$, $x_n$ represent
the $n$ observed values.
\end{onebox}

\begin{exercisewrap}
\begin{nexercise}
Examine the equation for the mean.
What does $x_1$ correspond to? And $x_2$?
Can you infer a general meaning to what $x_i$
might represent?\footnotemark{}
\end{nexercise}
\end{exercisewrap}
\footnotetext{$x_1$ corresponds to the
  interest rate for the first loan in the sample (\loanA\%),
  $x_2$ to the second loan's interest rate (\loanB\%),
  and $x_i$ corresponds to the interest rate for the
  $i^{th}$ loan in the data set.
  For example, if $i = 4$, then we're examining $x_4$,
  which refers to the fourth observation in the data set.}

\begin{exercisewrap}
\begin{nexercise}
What was $n$ in this sample of
loans?\footnotemark{}
\end{nexercise}
\end{exercisewrap}
\footnotetext{The sample size was $n = 50$.}

The \data{loan50} data set represents a sample from
a larger population of loans made through Lending Club.
We could compute a mean for this population in the same way
as the sample mean.
However, the population mean has a special label: $\mu$.
\index{Greek!mu@mu ($\mu$)}
The symbol $\mu$ is the Greek letter \emph{mu} and represents
the average of all observations in the population.
Sometimes a subscript, such as $_x$,
is used to represent which variable the population mean
refers to, e.g. $\mu_x$.
Often times it is too expensive to measure the
population mean precisely, so we often estimate
$\mu$ using the sample mean, $\bar{x}$.

\begin{examplewrap}
\begin{nexample}{The average interest rate across all loans
    in the population can be estimated using the sample data.
    Based on the sample of 50 loans,
    what would be a reasonable estimate of $\mu_x$,
    the mean interest rate for all loans in the
    full data set?}
  The sample mean, \loanAvg{}\%, provides a rough estimate
  of $\mu_x$.
  While it's not perfect, this is our single best guess
  %\emph{point estimate}\index{point estimate}
  of the average interest rate of all the loans in the
  population under study.

  In Chapter~\ref{ch_inference_for_props} and beyond,
  we will develop tools to characterize the accuracy
  of \emph{point estimates}\index{point estimate}
  like the sample mean.
  As you might have guessed,
  point estimates based on larger samples tend to be
  more accurate than those based on smaller samples.
\end{nexample}
\end{examplewrap}


%\begin{examplewrap}
%\begin{nexample}{We might like to compute the average income per person in the US.  The average income per person (per capita income) in each of the 3,144 counties in the US is published by the Census Bureau.  Say we average over these 3,144  values.  Do you think this will provide a good estimate of average income per person in the US?} \label{wtdMeanOfIncome}
%Each county actually represents many individual people. If we were to simply average across the counties, we would be treating counties with 5,000 and 5,000,000 residents equally in the calculations. Instead, we should compute a \emph{weighted} mean, where we weight or multiply each county's value by its population size, add up each of the terms, and then divide by the total number of people in all the counties in the US. 
%\end{nexample}
%\end{examplewrap}


The median provides another measure of center. The \term{median} splits an ordered data set in half. There are 50 interest rates in the \data{loan50} data set (an even number) so the data are perfectly split into two groups of~25. We take the median in this case to be the average of the two middle observations: $(\text{9.93} + \text{9.93}) / 2 = \text{9.93}$. When there are an odd number of observations, there will be exactly one observation that splits the data into two halves, and in this case that observation is the median (no average needed).

\begin{onebox}{Median: the number in the middle}
In an ordered data set, the \term{median} is the observation right in the middle. If there are an even number of observations, the median is the average of the two middle values.\end{onebox}

Graphically, we can think of the mean as the balancing point.  To estimate the median from a histogram, we try to split the \emph{area} in half so that 50\% of the area is to the left and 50\% of the area is to the right.



\begin{figure}[h]
   \centering
   \Figures [ A histogram with a horizontal axis of "Interest Rate" and a vertical axis showing the frequency of occurrence of different bins of interest rate. The first bin is from 5\%-7.5\% with a frequency (count) of 11 observations, 7.5\%-10\% has a frequency of 15, 10\%-12.5\% has 8, 12.5\%-15\% has 4, 15\%-17.5\% has 5, 17.5\%-20\% has 4, and then the 20\%-22.5\%, 22.5\%-25\%, and 25\%-27.5\% bins each have a frequency of 1. The median is indicated with a yellow line at approximately 10 and the mean is indicated with a red line at approximately 12.  The graph is right skewed, with the mean greater than the median.]
{0.8}{loan50IntRateHist}{loan50IntRateHistMeanMedian}
   \caption{A histogram of \var{interest\_\hspace{0.3mm}rate} with its mean and median shown.}
   \label{loan50IntRateHistWMeanMedian}
\end{figure}

\begin{examplewrap}
\begin{nexample}{Based on the data, why is the mean greater than the median in this data set?}
Consider the two largest values which are 25\% and 26\% . These values drag up the mean because they increase the sum (the~total). However, they do not drag up the median because their magnitude does not change the location of the middle value.
\end{nexample}
\end{examplewrap}

%\Comment{add box}.

\begin{onebox}{The mean follows the tail}
In a right skewed distribution, the mean is greater than the median.

In a left skewed distribution, the mean is less than the median.

In a symmetric distribution, the mean and median are approximately equal.\end{onebox}

\begin{exercisewrap}
\begin{nexercise}Consider the distribution of individual income in the United States. Which is greater: the mean or median? Why?\footnotemark
\end{nexercise}
\end{exercisewrap}
\footnotetext{Because a small percent of individuals earn extremely large amounts of money while the majority earn a modest amount, the distribution is skewed to the right. Therefore, the mean is greater than the median.}

\D{\newpage}
%%
\subsection{Standard deviation and IQR as measures of spread}
\label{variability}

The mean interest rate in the \data{loan50} data set is  \loanAvg{}\%.  Say that you are offered an interest rate of 15\%.  Should we consider this value to be far from the mean?  In order to answer this question, it is not enough to know the center of the data set and its \term{range} (computed as: maximum value - minimum value). We must know about the variability\index{variability} of the data set within that range. Low variability or small spread means that the values tend to be more clustered together. High variability or large spread means that the values tend to be far apart.

\begin{examplewrap}
\begin{nexample}{Is it possible for two data sets to have the same range but different spread? If so, give an example. If not, explain why not.}
Yes. An example is:  {1, 1, 1, 1, 1, 9, 9, 9, 9, 9} and {1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 9}.

The first data set has a larger spread because values tend to be farther away from each other, while in the second data set, most values are clustered together at the mean.
\end{nexample}
\end{examplewrap}

Here, we introduce the standard deviation as a measure of spread. Though its formula is a bit tedious to calculate by hand, the standard deviation is very useful in data analysis and roughly describes how far away, on average, the observations are from the mean.

We call the distance of an observation from its mean its \term{deviation}. Below are the deviations for the $1^{st}_{}$, $2^{nd}_{}$, $3^{rd}$, and $50^{th}_{}$ observations in the \var{interest\us{}rate} variable:
\begin{align*}
x_1^{}-\bar{x} &= \loanA - \loanAvg{} = \loanAdev \hspace{5mm}\text{ } \\
x_2^{}-\bar{x} &= \loanB - \loanAvg{} = \loanBdev \\
x_3^{}-\bar{x} &= \loanC - \loanAvg{} = \loanCdev \\
			&\ \vdots \\
x_{50}^{}-\bar{x} &= \loanZ - \loanAvg{} = \loanZdev
\end{align*}
If we square these deviations and then take an average,
the result is equal to the sample
\term{variance}\label{varianceIsDefined},
denoted by $s_{}^2$:
\begin{align*}
s_{}^2 &= \frac{(\loanAdev)_{}^2 + (\loanBdev)_{}^2 + (\loanCdev)_{}^2 + \cdots + (\loanZdev)_{}^2}{\loanN{}-1} \\
	&= \frac{0.45 + 2.72 + 216.97 + \cdots + 30.14}{49} \\
	&= \loanVar{}
\end{align*}
We divide by $n - 1$, rather than dividing by $n$,
when computing a sample's variance;
there's some mathematical nuance here, but the end result is that
doing this makes this statistic slightly more reliable and useful.

Notice that squaring the deviations does two things.
First, it makes large values relatively much larger,
seen by comparing $(\loanAdev)^2$, $(\loanBdev)^2$, $(\loanCdev)^2$,
and $(\loanZdev)^2$.
Second, it gets rid of any negative signs.

The \term{standard deviation} is defined as the square root of the variance:
\begin{align*}
s = \sqrt{\loanVar{}} = \loanSD{}
\end{align*}
While often omitted, a subscript of $_x$ may be added
to the variance and standard deviation,
i.e. $s_x^2$ and $s_x^{}$, if it is useful as a reminder
that these are the variance and standard deviation of the
observations represented by $x_1^{}$, $x_2^{}$, ..., $x_n^{}$.



\begin{onebox}{Calculating the standard deviation}
The standard deviation is the square root of the variance. It is roughly the ``typical" distance of the observations from the mean.
\begin{eqnarray*}
\label{sdEquation}
s_{\scriptscriptstyle{X}}
 = \sqrt{\frac{1}{n-1} \sum{(x_i -  \bar{x})^2}}
\end{eqnarray*}
\end{onebox}

The variance is useful for mathematical reasons, but the standard deviation is easier to interpret because it has the same units as the data set. The units for variance will be the units squared (e.g. meters$^2$).
Formulas and methods used to compute the variance and standard deviation for a population are similar to those used for a sample.\footnote{The only difference is that the population variance has a division by $n$ instead of $n-1$.} However, like the mean, the population values have special symbols: $\sigma_{}^2$ for the variance and $\sigma$ for the standard deviation. The symbol $\sigma$ \index{Greek!sigma@sigma ($\sigma$)} is the Greek letter \emph{sigma}.

\begin{onebox}{thinking about the standard deviation}
It is useful to think of the standard deviation as the ``typical" or ``average" distance that observations fall from the mean.\end{onebox}


\begin{figure}[h]
  \centering
  \Figure
    [A dot plot of 50 observations is shown with values ranging from about 5\% to 26\%. The data set is the same as that shown in the dot plot in Figure~\ref{loan_int_rate_dot_plot}, where the data is more dense from 5\% to about 11\%, has medium density from about 12\% to 20\%, and then there are a few more values scattered in the 20\% to 27\% range. Shading is shown to represent the regions within 1, 2, and 3 standard deviations. The region within 1 standard deviation is from 6.5\% to 16.7\%, representing 34 of the 50 data points. The region within 2 standard deviation runs left off of the chart (but would be from about 1.4\%) to 21.8\% and contains 48 of the 50 data points. The third standard deviation is shown to extend out to 26.9\%, and all 50 observations are contained within the 3 standard deviations.]
    {0.7}{sdRuleForIntRate}
  \caption{For the \var{interest\us{}rate} variable,
      34 of the 50 loans (68\%) had interest rates within
      1~standard deviation of the mean,
      and 48 of the 50 loans (96\%) had rates within
      2~standard deviations.
      Usually about 70\% of the data are within 1~standard
      deviation of the mean and 95\% within 2~standard
      deviations, though this is far from a hard rule.\D{\vspace{-5mm}}}
  \label{sdRuleForIntRate}
\end{figure}



\begin{examplewrap}
\begin{nexample}{Given that the average interest rate for the \data{loan50} data set is \loanAvg{}\%, with a standard deviation of \loanSD{}\%, is an interest rate of 15\% especially far from the mean?  }
Because 15\% is within one standard deviation of the mean, it is not especially far from the mean. If the value were more than 2 standard deviations away from the mean, we would consider it far from the mean.
\end{nexample}
\end{examplewrap}

In Chapter~\ref{ch_probability}, we encounter a bell-shaped distribution known as the \emph{normal distribution}.  The \term{empirical rule} tells us that for nearly normal distributions, about 68\% of the data will be within one standard deviation of the mean, about 95\% will be within two standard deviations of the mean, and about 99.7\% will be within three standard deviations of the mean. However, as seen in Figure~\ref{severalDiffDistWithSdOf1}, these percentages generally do not hold if the distribution is not bell-shaped.  

%\D{\newpage}

\begin{figure}
\centering
\Figure
    [Three histograms are shown (upper, middle, lower). Each distribution also shows shading -- dark gray between -1 to 1, lighter gray between -2 and 2, and light gray between -3 and 3, and then very light gray further out. The upper plot shows only two bins with non-zero values and of equal height at -1 and 1. middle plot shows a bell-shaped curve, where most of the higher bin values are between -1 and 1, middling heights are between -2 to -1 and 1 to 2, and the data trails off in each direction with ever-smaller values further out. The lower histogram shows no data below about -1.6, a quick increase to a peak at about -0.7 and then a slow decline of values to about half the max height at 1 and further trails off to ever smaller values to a horizontal location of 3 and beyond.]
{0.7}{severalDiffDistWithSdOf1}
\caption{Three very different population distributions with the same mean $\mu=0$ and standard deviation $\sigma=1$.}
\label{severalDiffDistWithSdOf1}
\end{figure}

\begin{exercisewrap}
\begin{nexercise}
On page~\pageref{shapeFirstDiscussed}, the concept of shape of a distribution was introduced. A good description of the shape of a distribution should include modality and whether the distribution is symmetric or skewed to one side. Using Figure~\ref{severalDiffDistWithSdOf1} as an example, explain why such a description is important.\footnotemark
\end{nexercise}
\end{exercisewrap}
\footnotetext{Figure~\ref{severalDiffDistWithSdOf1} shows three distributions that look quite different, but all have the same mean, variance, and standard deviation. Using modality, we can distinguish between the first plot (bimodal) and the last two (unimodal). Using skewness, we can distinguish between the last plot (right skewed) and the first two. While a graph tells a more complete story, we can use modality and shape (symmetry/skew) to characterize basic information about a~distribution.}


In this chapter we use standard deviation as a descriptive statistic to describe the variability in a given data set. In later chapters, we will use standard deviation to assess how closely a sample proportion is likely to be to the population proportion and how closely a sample mean is likely to be to the population mean.

Another measure of spread involves looking at the range of the middle 50\% of the data in a data set.  $Q_1$\index{quartile!first quartile (Q$_1$)} represents the \term{first quartile}, which is the 25th percentile, and is the median of the smaller half of the data set.  $Q_3$\index{quartile!third quartile (Q$_3$)} represents the \term{third quartile}, or 75th percentile, and is the median of the larger half of the data set.  We calculate the variability in the data using the range of the middle 50\% of the data: $Q_3 - Q_1$. This quantity is called the \termsub{interquartile range}{interquartile range (IQR)} (IQR, for short). It, like the standard deviation, is a measure of \indexthis{variability}{variability} or \term{spread} in data. The more variable the data, the larger the standard deviation and~IQR tend to be.

In the \data{loan50} data set, there are 50 values, so there are 25 values in the lower and in the upper half of the data set.  $Q_1$ is the median of the lower 25 values, and so is the middle or 13th value.  In this case, $Q_1$ is  \loanQA{}\%.  $Q_3$ is the median of the upper 25 values, and is the 38th value, which is  \loanQC{}\%.

\begin{onebox}{Interquartile range (IQR)}
The IQR\index{interquartile range (IQR)} is computed as
\begin{eqnarray*}
IQR = Q_3 - Q_1
\end{eqnarray*}
where $Q_1$ and $Q_3$ are the $25^{th}$ and $75^{th}$ percentiles.\end{onebox}

\begin{examplewrap}
\begin{nexample}{Calculate the IQR of interest rate for the \data{loan50} data set.  How does this compare to the previously calculated standard deviation of interest rate?}
The  $IQR = Q_3 - Q_1 =  \loanQC{} -  \loanQA{} = \loanIQR{}$.  The IQR of interest rate is \loanIQR{}\%, while the standard deviation of interest rate was \loanSD{}\%.  In general, these two measures of spread will yield different values.
\end{nexample}
\end{examplewrap}


\D{\newpage}

%%
\subsection{Linear transformations of data and changing units}
\label{linearTransformationOfData}

\begin{examplewrap}
\begin{nexample}{Begin with the following list:  {1, 1, 5, 5}. Multiply all of the numbers by 10. What happens to the mean? What happens to the standard deviation? How do these compare to the mean and the standard deviation of the original list?}
The original list has a mean of 3 and a standard deviation of 2. The new list: {10, 10, 50, 50} has a mean of 30 with a standard deviation of 20. Because all of the values were multiplied by 10, both the mean and the standard deviation were multiplied by~10.~\footnotemark
\end{nexample}
\end{examplewrap}
\footnotetext{Here, the population standard deviation was used in the calculation. These properties can be proven mathematically using properties of sigma (summation).}

\begin{examplewrap}
\begin{nexample}{Start with the following list:  {1, 1, 5, 5}. Multiply all of the numbers by \mbox{-0.5}. What happens to the mean? What happens to the standard deviation? How do these compare to the mean and the standard deviation of the original list?}
The new list: {-0.5, -0.5, -2.5, -2.5} has a mean of -1.5 with a standard deviation of~1. Because all of the values were multiplied by~\mbox{-0.5}, the mean was multiplied by~\mbox{-0.5}. Multiplying all of the values by a negative flipped the sign of numbers, which affects the location of the center, but not the spread. Multiplying all of the values by \mbox{-0.5} multiplied the standard deviation by +0.5 because the standard deviation cannot be negative.
\end{nexample}
\end{examplewrap}

\begin{examplewrap}
\begin{nexample}{Again, start with the following list: {1, 1, 5, 5}. Add 100 to every entry. How do the new mean and standard deviation compare to the original mean and standard deviation?}
The new list is: {101, 101, 105, 105}. The new mean of 103 is 100 greater than the original mean of 3. The new standard deviation of 2 is the \emph{same} as the original standard deviation of 2. Adding a constant to every entry shifted the values, but did not stretch or contract them.
\end{nexample}
\end{examplewrap}

Suppose that a researcher is looking at a list of 500 temperatures recorded in Celsius~(C). The mean of the temperatures listed is given as 27\degree C with a standard deviation of 3\degree C. Because she is not familiar with the Celsius scale, she would like to convert these summary statistics into Fahrenheit~(F). To convert from Celsius to Fahrenheit, we use the following conversion:
\begin{align*}
x_{_F} = \frac{9}{5}x_{_C} + 32
\end{align*}
Fortunately, she does not need to convert each of the 500 temperatures to Fahrenheit and then recalculate the mean and the standard deviation. The unit conversion above is a linear transformation of the following form, where $a=9/5$ and $b=32$:
\begin{align*}
aX + b
\end{align*}
\D{\newpage}
\noindent{}Using the examples as a guide, we can solve this temperature-conversion problem. The mean was 27\degree C and the standard deviation was 3\degree C. To convert to Fahrenheit, we multiply all of the values by $9/5$, which multiplies both the mean and the standard deviation by $9/5$. Then we add 32 to all of the values which adds 32 to the mean but does not change the standard deviation further.
\begin{align*}
\bar{x}_{F} &= \frac{9}{5}\bar{x}_{C} + 32 & \sigma_{F} &= \frac{9}{5}\sigma_{C} \\
&= \frac{5}{9}(27)+ 32 & &=\frac{9}{5}(3) \\
&=80.6 &  &=5.4
\end{align*}


\begin{figure}[h]
   \centering
\Figure[Two histograms are shown, graphing 500 temperatures in Celsius on the left, and the same 500 temperatures in Fahrenheit on the right.  This illustrates how converting from Celsius to Fahrenheit involves a shift and a stretch.  The graph in Celsius has a mean of 27 degrees C and a standard deviation of 3 degrees C.  The graph in Fahrenheit has a mean of 80.6 degrees F and a standard deviation of 5.4 degrees F.]
{0.9}{CToF_ConversionFigure}
   \caption{500 temperatures shown in both Celsius and \mbox{Fahrenheit.}}
   \label{CToF_ConversionFigure}
\end{figure}


\begin{onebox}{Adding shifts the values, multiplying stretches or contracts them}
Adding a constant to every value in a data set shifts the mean but does not affect the standard deviation. Multiplying the values in a data set by a constant will change the mean and the standard deviation by the same multiple, except that the standard deviation will always remain positive.\end{onebox}

\begin{examplewrap}
\begin{nexample}{Consider the temperature example. How would converting from Celsuis to Fahrenheit affect the median? The IQR?}
The median is affected in the same way as the mean and the IQR is affected in the same way as the standard deviation. To get the new median, multiply the old median by $9/5$ and add 32. The IQR is computed by subtracting $Q_1$ from $Q_3$. While $Q_1$ and $Q_3$ are each affected in the same way as the median, the additional 32 added to each will cancel when we take $Q_3 - Q_1$. That is, the IQR will be increase by a factor of $9/5$ but will be unaffected by the addition of~32.

For a more mathematical explanation of the IQR calculation, see the footnote.\footnotemark
\end{nexample}
\end{examplewrap}
\footnotetext{New IQR = $\left(\frac{9}{5} Q_3 + 32\right) - \left(\frac{9}{5} Q_1 + 32\right) = \frac{9}{5} \left(Q_3 - Q_1\right) = \frac{9}{5} \times \text{(old IQR)}$.}


\D{\newpage}

%%
\subsection{Outliers and robust statistics}
\index{outlier}

Why do we have two measures of spread?  As with median, the IQR is more robust, that is, more resistant to outliers.  While the IQR only looks at the 25th and 75th percentile values, the standard deviation calculation involves all the data points and so can be inflated by extremely small or extremely large values. 

Here we present two common rules of thumb for identifying outliers.

\begin{onebox}{Rules of thumb for identifying outliers}
There are two rules of thumb for identifying outliers in a numerical data set:
\begin{itemize}
\setlength{\itemsep}{0mm}
\item More than 1.5$\times$ IQR below $Q_1$ or above $Q_3$
\item More than 2 standard deviations above or below the mean.
\end{itemize}
Both are important for the AP exam. In practice, consider these to be only rough guidelines.\end{onebox}

\begin{exercisewrap}
\begin{nexercise}For the \data{loan50} data set, $Q_1=$ \loanQA{}  and $Q_3=\loanQC{} $. $\bar{x}$ = \loanAvg{} and $s$ = \loanSD{}. What values would be considered an outlier on the low end using each rule?\footnotemark
\end{nexercise}
\end{exercisewrap}
\footnotetext{ $Q_1 - 1.5\times IQR = \loanQA{} - 1.5 \times (\loanQC{} - \loanQA{}) = \loanIQR{}$, so values less than -1.22\% would be considered an outlier using the first rule of thumb. Using the second rule of thumb, a value less than $\bar{x} - 2\times s = \loanAvg{} - 2 \times \loanSD{} = 1.47$ would be considered an outlier. Note that these are just rules of thumb and usually yield different values.}


\begin{exercisewrap}
\begin{nexercise} Examine the distribution of the interest rates in the \data{loan50} data set.  What does the fact that there are outliers on the high end but not on the low end suggestion?\footnotemark
\end{nexercise}
\end{exercisewrap}
\footnotetext{It suggests that the distribution has a right hand tail, that~is, that it is right skewed.}

How are the \indexthis{sample statistics}{sample statistic}
of the \data{interest\us{}rate} data set affected
by the observation, 26.3\%?
What would have happened if this loan had instead
been only 15\%?
What would happen to these
\indexthis{summary statistics}{summary statistic}
if the observation at 26.3\% had been even larger,
say 35\%? These scenarios are plotted alongside the
original data in Figure~\ref{loan_int_rate_robust_ex},
and sample statistics are computed under each scenario in
Figure~\ref{robustOrNotTable}.

\begin{figure}[ht]
  \centering
  \Figure
    [Three dot plots are shown in the same plot. The largest observation from the original data set (discussed in previous dot plots) at about 26\% is moved to 15\% in the second dot plot and instead to 35\% in the third dot plot.]
    {1}{loan_int_rate_robust_ex}
  \caption{Dot plots of the original interest rate data
      and two modified data sets.}
  \label{loan_int_rate_robust_ex}
\end{figure}

\D{\newpage}

% See `loan_int_rate_robust_ex` figure code for calculations.
\captionsetup{width=135mm}
\begin{figure}[ht]
\centering
\begin{tabular}{l c cc c cc}
% \cline{3-4} \cline{6-7}
& \hspace{0mm} & \multicolumn{2}{c}{\bf robust} &
    \hspace{2mm} & \multicolumn{2}{c}{\bf not robust} \\
\hline
scenario && median & IQR && $\bar{x}$ & $s$ \\ 
\hline
%   & & \multicolumn{2}{c|} & & \multicolumn{2}{c|} \\
original \var{interest\us{}rate} data
    && 9.93\% & 5.76\% && 11.57\% & 5.05\% \\
move 26.3\% $\to$ 15\%
    && 9.93\% & 5.76\% && 11.34\% & 4.61\% \\
move 26.3\% $\to$ 35\%
    && 9.93\% & 5.76\% && 11.74\% & 5.68\% \\
   \hline
\end{tabular}
\caption{A comparison of how the median, IQR,
  mean ($\bar{x}$), and standard deviation ($s$) change
  had an extreme observations from the \var{interest\us{}rate}
  variable been different.}
\label{robustOrNotTable}
\end{figure}
\captionsetup{width=\mycaptionwidth}

\begin{exercisewrap}
\begin{nexercise} \label{interestRateWhichIsMoreRobust}
(a)~Which is more affected by extreme observations,
the mean or median?
Figure~\ref{robustOrNotTable} may be helpful.
(b)~Is the standard deviation or IQR more affected by
extreme observations?\footnotemark{}
\end{nexercise}
\end{exercisewrap}
\footnotetext{(a)~Mean is affected more.
  (b)~Standard deviation is affected more.
  Complete explanations are provided in the material
  following Guided Practice~\ref{interestRateWhichIsMoreRobust}.}

The median and IQR are called \term{robust statistics} because
extreme observations have little effect on their values:
moving the most extreme value generally has little influence
on these statistics.
On the other hand, the mean and standard deviation
are more heavily influenced by changes in extreme observations,
which can be important in some situations.

\begin{examplewrap}
\begin{nexample}{The median and IQR did not change under the
    three scenarios in Figure~\ref{robustOrNotTable}.
    Why might this be the case?}
  The median and IQR are only sensitive to numbers
  near $Q_1$, the median, and $Q_3$.
  Because values in these regions are stable in the three
  data sets, the median and IQR estimates are also stable.
\end{nexample}
\end{examplewrap}

\begin{exercisewrap}
\begin{nexercise}
The distribution of loan amounts in the \data{loan50} data set
is right skewed, with a few large loans lingering out into the
right tail.
If you were wanting to understand the typical loan size,
should you be more interested in the mean
or median?\footnotemark
\end{nexercise}
\end{exercisewrap}
\footnotetext{Answers will vary!
  If we're looking to simply understand what a typical individual
  loan looks like, the median is probably more useful.
  However, if the goal is to understand something that
  scales well, such as the total amount of money we might
  need to have on hand if we were to offer 1,000 loans,
  then the mean would be more useful.}

\index{data!loan50|)}


\D{\newpage}

%%
\subsection{Box plots}

A \term{box plot} summarizes a data set using five
statistics while also plotting unusual observations.
Figure~\ref{loan_int_rate_box_plot_layout} provides
a vertical dot plot alongside a box plot of the
\var{interest\us{}rate} variable from
the \data{loan50} data set.

\begin{figure}[h]
  \centering
  \Figure
    [What is shown in a a dot plot adjacent to what is called a "box plot". The data values are the same ones used in past dot plots, where the data shows greatest density from 5\% to 11\%, moderate density from 12\% to 20\%, and then a few more values at about 22\%, 25\%, and 26\%. The box plot adjacent to the data shows a box that would encapsulate the middle 50\% of the data, from about 8\% to 13\%. The median is also annotated with a line through the center of the box. From here, the data extend out with "whiskers" up to a distance up to $1.5 \times IQR$ below and above the box to capture as much data as possible. There are two observations that extend beyond this range at 25\% and 26\%.]
    {0.8}{loan_int_rate_box_plot_layout}
  \caption{A vertical dot plot, where points have been
      horizontally stacked, next to a labeled box plot
      for the interest rates of the \loanN{} loans.}
  \label{loan_int_rate_box_plot_layout}
\end{figure}

The five summary statistics used in a box plot are known as the \term{five-number summary}, which consists of the minimum, the maximum, and the three quartiles ($Q_1$, $Q_2$, $Q_3$) of the data set being studied.  Recall that $Q_1$ represents the 25th percentile and $Q_3$ represents the 75th percentile.  $Q_2$ represents the 50th percentile, also known as the median.
 

To build a box plot, draw an axis (vertical or horizontal) and draw a scale. Draw a dark line denoting $Q_2$, the median. Next, draw a line at $Q_1$ and at $Q_3$. Connect the $Q_1$ and $Q_3$ lines to form a rectangle. The width of the rectangle corresponds to the IQR and the middle 50\% of the data is in this interval.

\begin{onebox}{Outliers in the context of a box plot}
When in the context of a box plot, define an \term{outlier} as an \mbox{observation} that is more than $1.5 \times IQR$ above $Q_3$ or $1.5 \times IQR$ below $Q_1$. Such points are marked using a dot or asterisk in a box plot.\end{onebox}

Extending out from the rectangle, the \term{whiskers} attempt to capture all of the data remaining outside of the box, except outliers. In Figure~\ref{loan_int_rate_box_plot_layout}, the upper whisker does not extend to the last two points, which are beyond $Q_3 + 1.5\times IQR$ and are outliers, so it extends only to the last point below this limit.\footnote{You might wonder, isn't the choice of $1.5 \times IQR$ for defining an outlier arbitrary? It is! In practical data analyses, we tend to avoid a strict definition since what is an unusual observation is highly dependent on the context of the data.} The lower whisker stops at the lowest value, 5, because there are no additional data to reach. Outliers are each marked with a dot or asterisk. In a sense, the box is like the body of the box plot and the whiskers are like its arms trying to reach the rest of the data.

Box plots may be oriented vertically as seen in Figure~\ref{loan_int_rate_box_plot_layout} or horizontally as seen in Figure~\ref{loan_int_rate_box_plot_horiz}.  In both cases, the box plot and the information it tells us is identical, with the only difference being the axis that the variable is graphed along.
\begin{figure}[h]
  \centering
  \Figures
    [A box plot of the interest rate variable, with the variable along the horizontal axis.]
    {0.33}{loan_int_rate_box_plot_layout}{loan_int_rate_box_plot_horiz}
  \caption{A box plot for interest rate, with the variable along the horizontal axis.}
  \label{loan_int_rate_box_plot_horiz}
\end{figure}


\begin{exercisewrap}
\begin{nexercise}
Using Figure~\ref{loan_int_rate_box_plot_horiz},
estimate the following values for
\var{interest\us{}rate} in the \data{loan50} data set: \\
(a) $Q_1$,
(b) $Q_3$, and
(c) IQR.\footnotemark{}
\end{nexercise}
\end{exercisewrap}
\footnotetext{These
  visual estimates will vary a little from one person
  to the next:
  $Q_1=$ 8\%,
  $Q_3=$ 14\%,
  $\text{IQR} = Q_3 - Q_1 = 6\%$.
  (The true values: $Q_1= \loanQA{}\%$, $Q_3 = \loanQC{}\%$,
  $\text{IQR} = \loanIQR{}\%$.)}


\begin{examplewrap}
\begin{nexample}{Compare the box plot to the graphs previously discussed: stem-and-leaf plot, dot plot, frequency and relative frequency histogram. What can we learn more easily from a box plot? What can we learn more easily from the other graphs?}
It is easier to immediately identify the quartiles from a box plot. The box plot also more prominently highlights outliers. However, a box plot, unlike the other graphs, does not show the \emph{distribution} of the data. For example, we cannot generally identify modes using a box plot.
\end{nexample}
\end{examplewrap}

\begin{examplewrap}
\begin{nexample}
{Is it possible to identify skew from the box plot?} Yes. Looking at the lower and upper whiskers of this box plot, we see that the lower 25\% of the data is squished into a shorter distance than the upper 25\% of the data, implying that there is greater density in the low values and a tail trailing to the upper values. This box plot is right skewed.
\end{nexample}
\end{examplewrap}

\begin{exercisewrap}
\begin{nexercise}
True or false: there is more data between the median and $Q_3$ than between $Q_1$ and the median.\footnotemark
\end{nexercise}
\end{exercisewrap}
\footnotetext{False. Because $Q_1$ is the 25th percentile and the median is the 50th percentile, 25\% of the data fall between $Q_1$ and the median. Similarly, 25\% of the data fall between $Q_2$ and the median. The distance between the median and $Q_3$ is larger because that 25\% of the data is more spread out.}

\begin{examplewrap}
\begin{nexample}
{Consider the following ordered data set.
\begin{center}
\begin{tabular}{ccc ccc ccc}
5 & 5 & 9 & 10 & 15 & 16 & 30 & 40 & 80 
\end{tabular}
\end{center}
Find the five-number summary and identify how small or large a value would need to be to be considered an outlier.  Are there any outliers in this data set?}
There are nine numbers in this data set.  Because $n$ is odd, the median is the middle number: 15.  When finding $Q_1$, we find the median of the lower half of the data, which in this case includes 4 numbers (we do not include the 15 as belonging to either half of the data set).  $Q_1$ then is the average of 5 and 9, which is $Q_1 = 7$, and $Q_3$ is the average of 30 and 40, so $Q_3 = 35$.  The min is 5 and the max is 80.  To see how small a number needs to be to be an outlier on the low end we do:
\begin{align*}
Q_1 - 1.5 \times IQR
	&= Q_1 - 1.5 \times (Q_3 - Q_1) \\
	& = 7 - 1.5 \times (35 - 7) \\
	& = -35
\end{align*}
On the high end we need:
\begin{align*}
Q_3 + 1.5 \times IQR
	& = Q_3 + 1.5 \times (Q_3-Q_1) \\
	& = 35 + 1.5 \times (35 - 7) \\
	& = 77
\end{align*}
There are no numbers less than -41, so there are no outliers on the low end. The observation at 80 is greater than 77, so 80 is an outlier on the high end.
\end{nexample}
\end{examplewrap}

%\Comment{TODO(David)  create another box plot using different data set}
%\Comment{TODO(Leah) redo exercise}

%%
\subsection{Comparing numerical data across groups}
\label{comparingAcrossGroups}




Some of the more interesting investigations can be considered by examining numerical data across groups. The methods required here aren't really new. All that is required is to make a numerical plot for each group. To make a direct comparison between two groups, create a pair of dot plots or a pair of histograms drawn using the same scales. It is also common to use back-to-back stem-and-leaf plots and parallel box plots, all of which are explored here.

The \data{loan50} data set is a sample from the much larger data set \data{loans\us{}full\us{}schema}.  To compare interest rates across the individual and joint application types, we take a look at a random sample of 50 individual loans and a random sample of 40 joint loans from the \data{loans\us{}full\us{}schema} data set.  The interest rates for each loan are shown in Figure~\ref{incomeSplitByLoanType}, separated by application type.

\newcommand{\npgpad}[1]{\hspace{2mm}#1\hspace{1.5mm}\ }
\begin{figure}[h]
\begin{center}
\begin{tabular}{ ccc ccc c ccc }
\multicolumn{10}{c}{\bf Interest rate for randomly sampled loans} \\
\hline
\vspace{-2mm} \\
\multicolumn{5}{c}{\bf Individual} &\hspace{5mm}\ &
    \multicolumn{4}{c}{\bf Joint} \\ 
  \cline{1-5} \cline{7-10}
14&14&20&14&14 &&
    \npgpad{7} & \npgpad{6} & \npgpad{12}& \npgpad{15} \\
9&13&19&11&13&&20&20&8&22\\
7&11&14&14&18&&9&10&10&5\\
15&13&19&16&12&&13&16&8&17\\
17&6&15&14&17&&10&23&9&15\\
15&14&12&6&7&&27&12&15&15\\
8&14&17&15&16&&15&7&26&6\\
12&7&31&12&7&&14&18&21&14\\
13&17&12&10&16&&21&10&12&31\\
12&7&12&7&16&&30&14&15&12\\
\cline{1-5} \cline{7-10}
\end{tabular}
\caption{Left: interest rate for a random sample of 50 individual loan. Right: interest rate for a random sample of 40 joint loans.}
\label{incomeSplitByLoanType}
\end{center}
\end{figure}

In order to compare the distribution of interest rate for each loan type, we start with a \term{back-to-back stem-and-leaf plot}, shown in Figure~\ref{backtobackstemandleaf}.   As with a stem-and-leaf plot, a back-to-back stem-and-leaf plot is useful when the data sets have a small number of values.  The leaf values should get larger as you move farther away from the central stem.  Here, the stem represents the tens place, the leaf represents the ones place, and we split the stems to stretch them out. \begin{figure}[h]
\begin{verbatim}
                          Individual loans        Joint loans

                                 9877777766| 0 |566778899
                     4443333332222111111100| 1 |000022223444
                            998777766665555| 1 |555555678
                                          0| 2 |001123
                                           | 2 |67
                                          0| 3 |01
               	
                       Legend: 3 |1 = 31% interest rate
\end{verbatim}
\caption{Back-to-back stem-and-leaf plot for interest rate, split by whether the loan type was individual or joint.}
\label{backtobackstemandleaf}
\end{figure}


In addition to a back-to-back stem-and-leaf plot, we can use two dot plots or two histograms vertically stacked on the same scale for easy comparison.  These are shown in Figure~\ref{incomeSplitByLoanTypeDotHist}. Another option is to use parallel box~plots, as shown in Figure~\ref{incomeSplitByLoanTypeBoxPlot}.
\begin{figure}[h]
\Figures[]{.5}{interestRateSplitByType}{interestRateSplitByTypeDotPlot}
\Figures[]{.5}{interestRateSplitByType}{interestRateSplitByTypeHistogram}
\caption{Vertically stacked graphs comparing distribution of interest rate for individual versus joint loans. Left: stacked dot plots; Right:  stacked histograms.}
\label{incomeSplitByLoanTypeDotHist}
\end{figure}


\begin{figure}[h]
\begin{center}
\Figures[]{.6}{interestRateSplitByType}{interestRateSplitByTypeBoxplot}
\caption{Parallel box plots comparing 5-number summary of interest rate for individual versus joint loans.}
\label{incomeSplitByLoanTypeBoxPlot}
\end{center}
\end{figure}

\begin{examplewrap}
\begin{nexample}{What are benefits or drawbacks to using a back-to-back stem-and-leaf, stacked dot plots, stacked histograms, or parallel box plots to compare a variable across groups?}
The back-to-back stem-and-leaf and the stacked dot plots give us the most detail, because we can see every individual value.  However, these are only practical for small data sets.  The stacked histograms are good for comparing the distributions of larger data sets.  Parallel box plots do not show us the details of the distributions, but they are good for quickly comparing the five-number summaries and are particularly convenient for comparing across more than two groups.
\end{nexample}
\end{examplewrap}

\begin{examplewrap}
\begin{nexample}{Use the parallel box plots to compare the distribution of interest rate between individual and joint loans.}
The distributions of interest rate for individual and joint loans have a similar median (center), with the median interest rate for joint loans being just slightly higher.  The ranges are similar, but the interest rate for joint loans has a larger IQR, meaning more spread in the middle 50\% of the data.  Both distributions are right skewed.  The interest rates for individual loans has a gap between about 20\% and 30\%, with the largest value being an outlier.  The interest rates for joint loans have no outliers.
\end{nexample}
\end{examplewrap}

\begin{onebox}{Comparing distributions}
When comparing distributions, compare them with respect to center, spread, and shape as well as any unusual observations. Also, include contextual information about the variable.\end{onebox}

%\begin{figure}[h]
%\begin{center}
%Interest rates for 50 randomly sampled Individual loans\\
%\begin{tabular}{r r r r r r r r r r}
%14&14&20&14&14\\
%9&13&19&11&13\\
%7&11&14&14&18\\
%15&13&19&16&12\\
%17&6&15&14&17\\
%15&14&12&6&7\\
%8&14&17&15&16\\
%12&7&31&12&7\\
%13&17&12&10&16\\
%12&7&12&7&16
%\end{tabular}\\
%
%Interest rates for 40 randomly sampled Joint loans\\
%\begin{tabular}{r r r r r r r r r r}
%7&6&12&15\\
%20&20&8&22\\
%9&10&10&5\\
%13&16&8&17\\
%10&23&9&15\\
%27&12&15&15\\
%15&7&26&6\\
%14&18&21&14\\
%21&10&12&31\\
%30&14&15&12
%\end{tabular}
%\end{center}
%\end{figure}

%We will take a look again at the \data{county} data set
%and compare the median household income for counties that
%gained population from 2010 to 2017 versus counties that
%had no gain.
%
%
%\newcommand{\numcountieswithgains}{1454}
%\newcommand{\numcountieswithgainsC}{1,454}
%\newcommand{\numcountieswithoutgains}{1672}
%\newcommand{\numcountieswithoutgainsC}{1,672}
%
%There were \numcountieswithgainsC{} counties where
%the population increased from 2010 to 2017, and there
%were \numcountieswithoutgainsC{} counties with no gain
%(all but one were a loss).
%A~random sample of 100 counties from the first group and
%50 from the second group are shown in
%Figure~\ref{countyIncomeSplitByPopGainTable}
%to give a better sense of some of the raw median
%income data.
%
%\newcommand{\npgpad}[1]{\hspace{2mm}#1\hspace{1.5mm}\ }
%\begin{figure}
%\begin{center}
%\begin{tabular}{ ccc ccc c ccc }
%\multicolumn{10}{c}{\bf Median Income for 150 Counties,
%    in \$1000s} \\
%\hline
%\vspace{-2mm} \\
%\multicolumn{6}{c}{\bf Population Gain} &\hspace{5mm}\ &
%    \multicolumn{3}{c}{\bf No Population Gain} \\ 
%  \cline{1-6} \cline{8-10}
%38.2 & 43.6 & 42.2 & 61.5 & 51.1 & 45.7 &&
%    \npgpad{48.3} & \npgpad{60.3} & \npgpad{50.7} \\
%44.6 & 51.8 & 40.7 & 48.1 & 56.4 & 41.9 && 39.3 & 40.4 & 40.3 \\
%40.6 & 63.3 & 52.1 & 60.3 & 49.8 & 51.7 && 57 & 47.2 & 45.9 \\
%51.1 & 34.1 & 45.5 & 52.8 & 49.1 & 51 && 42.3 & 41.5 & 46.1 \\
%80.8 & 46.3 & 82.2 & 43.6 & 39.7 & 49.4 && 44.9 & 51.7 & 46.4 \\
%75.2 & 40.6 & 46.3 & 62.4 & 44.1 & 51.3 && 29.1 & 51.8 & 50.5 \\
%51.9 & 34.7 & 54 & 42.9 & 52.2 & 45.1 && 27 & 30.9 & 34.9 \\
%61 & 51.4 & 56.5 & 62 & 46 & 46.4 && 40.7 & 51.8 & 61.1 \\
%53.8 & 57.6 & 69.2 & 48.4 & 40.5 & 48.6 && 43.4 & 34.7 & 45.7 \\
%53.1 & 54.6 & 55 & 46.4 & 39.9 & 56.7 && 33.1 & 21 & 37 \\
%63 & 49.1 & 57.2 & 44.1 & 50 & 38.9 && 52 & 31.9 & 45.7 \\
%46.6 & 46.5 & 38.9 & 50.9 & 56 & 34.6 && 56.3 & 38.7 & 45.7 \\
%74.2 & 63 & 49.6 & 53.7 & 77.5 & 60 && 56.2 & 43 & 21.7 \\
%63.2 & 47.6 & 55.9 & 39.1 & 57.8 & 42.6 && 44.5 & 34.5 & 48.9 \\
%50.4 & 49 & 45.6 & 39 & 38.8 & 37.1 && 50.9 & 42.1 & 43.2 \\
%57.2 & 44.7 & 71.7 & 35.3 & 100.2 &  && 35.4 & 41.3 & 33.6 \\
%42.6 & 55.5 & 38.6 & 52.7 & 63 &  && 43.4 & 56.5 &  \\
%\cline{1-6} \cline{8-10}
%\end{tabular}
%\caption{In this table, median household income (in \$1000s)
%    from a random sample of 100 counties that had population
%    gains are shown on the left.
%    Median incomes from a random sample of 50 counties that
%    had no population gain are shown on the right.}
%\label{countyIncomeSplitByPopGainTable}
%\end{center}
%\end{figure}
%
%\begin{figure}
%\begin{verbatim}
%                         Population: Gain           Population: No Gain
%
%                                            | 2 |12                          
%                                            | 2 |79                          
%                                           4| 3 |1234                    
%                                 99999987555| 3 |5555799                 
%                             444433322111000| 4 |00111223333              
%                       999998887666666665555| 4 |55666666789                
%                       444333222221111110000| 5 |1112222                    
%                                887776666555| 5 |6677                       
%                                 33333222100| 6 |01                          
%                                           9| 6 |                             
%                                          42| 7 |                             
%                                          85| 7 |                             
%                                          21| 8 |    
%               
%                       Legend: 2 |1 = 21,000 median income
%\end{verbatim}
%\caption{Back-to-back stem-and-leaf plot for median income, split by whether the count had a population gain or no gain.}
%\label{stemandleafincomepopgainloss}
%\end{figure}
%
%The \term{side-by-side box plot}
%\index{box plot!side-by-side box plot}
%is a traditional tool for comparing across groups.
%An example is shown in the left panel of
%Figure~\ref{countyIncomeSplitByPopGain},
%where there are two box plots, one for each group,
%placed into one plotting window and drawn on the same scale.
%
%\begin{figure}
%  \centering
% \Figure[There are two figures shown: a side-by-side box plot on the left, and a two overlaid hollow histograms on the right. These two plots describe the same data for the "county" data set: a numerical variable for median household income and a categorical variable with levels of "gain" and "no gain" for the population change in the county. First, the side-by-side box plots shown as the left plot are described. This plot shows two box plots side-by-side, enclosed in the same general plot so they are close and so easier to compare. The left box plot represents "gain", and the right plot represents "no gain". The vertical axis runs from about \$20,000 to about \$130,000. Starting at the lower levels, the "no gain" lower whisker is at about \$20,000, while the "gain" lower whisker starts at about \$25,000. Each whisker runs upwards to the box, where the "no gain" box is reached first at about \$40,000 and the "gain" box at about \$47,000. The median line in each box is shown, where the "no gain" median is shown to at about \$45,000, even lower than the start of the "gain" box". The "gain" box's median is at about \$53,000 and is above the top of the "no gain box" at about \$52,000. The left "gain" box finally ends at about \$62,000. Above each box is the upper whisker. The upper whisker in the "gain" box plot extends far above that of the "no gain" box, reaching about \$87,000 vs \$70,000. Each box plot has many individual observations shown above the upper whisker. The largest outlier for "gain" is about \$130,000, and the largest outlier for "no gain" is about \$112,000. Next, moving onto the right plot of the two hollow histograms for the "gain" (in blue) and "no gain" (in red) categories. The hollow histograms are overlaid, making it easier to compare their shapes more directly. The histograms share a horizontal axis that runs from about \$20,000 up to about \$130,000. In each case, the histograms do not show the bins explicitly and instead only show the top portion of each histogram (hence the term "hollow histogram"), meaning each hollow histogram is described by a line outlining the top of each bin in each histogram. It is these lines that will be described. Starting at the left of the histograms, the "no gain" histogram line rises up slightly at \$20,000 before the "gain" histogram line starts rising starting at about \$25,000. The "no gain" line then ascends rapidly starting at about \$30,000, followed by the "gain" line ascending rapidly at about \$40,000, which is also about where the "no gain" category reaches a peak and holds steady until about \$50,000, which is also where the "gain" line has now peaked. It is at this \$50,000 point that the "no gain" line falls rapidly from what had been a relatively steady peak between about \$35,000 to \$50,000, with the "gain" group also much more slowly starting to descend at about \$50,000. At close to \$70,000, the "no gain" group is nearly touching the horizontal axis, while the "gain" group has only descended about 70\% of the way. The "no gain" group hovers close to horizontal axis until appearing indistinguishable from the horizontal axis a bit above \$90,000. On the other hand, the "gain" group shows a slow but steady decline from about 30\% of its peak at \$70,000 down to close to the horizontal axis at \$100,000. The "gain" category bumps up just a tiny amount between \$100,000 and \$130,000 before becoming indistinguishable from the horizontal axis.]
%{1.00}{countyIncomeSplitByPopGain}
%  \caption{Side-by-side box plot (left panel)
%      and hollow histograms (right panel) for
%      \var{med\us{}hh\us{}income},
%      where the counties are split by whether or not there was a population gain from 2010 to 2017.}
%  \label{countyIncomeSplitByPopGain}
%\end{figure}
%
%Another useful plotting method uses \termni{hollow histograms} to compare numerical data across groups. These are just the outlines of histograms of each group put on the same plot, as shown in the right panel of Figure~\ref{countyIncomeSplitByPopGain}.
%
%\begin{exercisewrap}
%\begin{nexercise} \label{comparingPriceByTypeExercise}
%Use the plots in Figure~\ref{countyIncomeSplitByPopGain}
%to compare the incomes for counties across the two groups.
%What do you notice about the approximate center of each group?
%What do you notice about the variability between groups?
%Is the shape relatively consistent between groups?
%How many \emph{prominent} modes are there for each
%group?\footnotemark{}
%\end{nexercise}
%\end{exercisewrap}
%\footnotetext{Answers may vary a little.
%  The counties with population gains tend to have higher
%  income (median of about \$45,000) versus counties without
%  a gain (median of about \$40,000).
%  The variability is also slightly larger for the population
%  gain group.
%  This is evident in the IQR, which is about 50\% bigger
%  in the \emph{gain} group.
%  Both distributions show slight to moderate right
%  skew
%  and are unimodal.
%  The box plots indicate there are many observations
%  far above the median in each group, though we should
%  anticipate that many observations will fall beyond
%  the whiskers when examining any data set that
%  contain more than a couple hundred data points.}


%\begin{exercisewrap}
%\begin{nexercise}
%What components of each plot in Figure~\ref{countyIncomeSplitByPopGain} do you find most useful?\footnotemark\end{nexercise}
%\end{exercisewrap}
%\footnotetext{Answers will vary. The parallel box plots are especially useful for comparing centers and spreads, while the hollow histograms are more useful for seeing distribution shape, skew, and groups of anomalies.}



%%
\subsection{Z-scores}
\label{zscores}


In the previous section, we compared the \emph{distribution} of a numerical variable across groups using graphs and summary statistics.  Here we investigate a quantity for comparing the relative position of \emph{individual} values between distributions or within a distribution.

We are often interested in knowing and comparing how far values are from the mean.  However, because we may want to compare values that are in different units, such as degrees Celsius and degrees Fahrenheit, it will be helpful to have a standardized way to compare distance from the mean.  Measuring the \emph{number of standard deviations} values are from the mean will allow us to compare relative distance from the mean even when values are in different units.  

The mean interest rate for the \data{loan50} data set was calculated as 11.57\%, with a standard deviation of 5.05\%.  The highest interest rate is about 26\%.  Is that value unusually high relative to the rest of the data set?  
The value 26\% is about 14.5\% above the mean, so it is between 2 and 3 standard deviations above the mean, meaning it is quite high relative to the rest of the data set.  This can be found by doing
  \begin{align*}
  \frac{26 - 11.57}{5.05} = 2.86
  \end{align*}

The number of standard deviations a value is above or below the mean is known as the \term{Z-score}.  A Z-score has no units, and therefore is sometimes also called \emph{standard units}\index{standard units}.

\begin{onebox}{The Z-score}
  The Z-score of an observation is the number of standard
  deviations it falls above or below the mean.
  We compute the Z-score for an observation $x$ that follows
  a distribution with mean $\mu$ and standard deviation
  $\sigma$ using
  \begin{align*}
  Z = \frac{x - \mu}{\sigma}
  \end{align*}
\end{onebox}

Observations above the mean always have positive Z-scores,
while those below the mean always have negative Z-scores.
If an observation is equal to the mean, then the Z-score is $0$.

\begin{examplewrap}
\begin{nexample} 
{Head lengths of brushtail possums have a mean of 92.6 mm and standard deviation 3.6 mm.
Compute the Z-scores for possums with head lengths of 95.4 mm
and 85.8~mm.}
\label{headLZScore}%
For $x_1=95.4$ mm:
\begin{align*}
    Z_1&= \frac{x_1 - \mu}{\sigma}\\
      &= \frac{95.4 - 92.6}{3.6}\\
      &= 0.78
\end{align*}
    For $x_2=85.8$ mm:
\begin{align*}
    Z_2 &= \frac{85.8 - 92.6}{3.6} \\
&= -1.89
\end{align*}
\end{nexample}
\end{examplewrap}


We can use Z-scores to roughly identify which observations
are more unusual than others.
An observation $x_1$ is said to be more unusual than another
observation $x_2$ if the absolute value of its Z-score is larger
than the absolute value of the other observation's Z-score:
$|Z_1| > |Z_2|$.
This technique is especially insightful when a distribution
is symmetric.

\begin{exercisewrap}
\begin{nexercise}
Which of the observations in Example~\ref{headLZScore}
is more unusual?\footnotemark
\end{nexercise}
\end{exercisewrap}
\footnotetext{Because the \emph{absolute value} of Z-score
  for the second observation ($x_2=85.8$ mm $\rightarrow Z_2=-1.89$) is larger than that of the first ($x_1=95.4$ mm  $\rightarrow Z_1=0.78$),
  the second observation has a more unusual head length.}

\begin{exercisewrap}
\begin{nexercise}
Let $X$ represent a random variable from a distribution with $\mu=3$ and $\sigma=2$,
and suppose we observe $x=5.19$. \\
%\begin{enumerate}[(a)]
%\setlength{\itemsep}{0mm}
%\item
(a)
    Find the Z-score of $x$. \\
%\item
(b)
    Interpret the Z-score.\footnotemark
%\end{enumerate}
\end{nexercise}
\end{exercisewrap}
\footnotetext{(a) Its Z-score is given by
    $Z
      = \frac{x-\mu}{\sigma}
      = \frac{5.19 - 3}{2}
      = \frac{2.19}{2}
      = 1.095$.
    (b)~The observation $x$ is 1.095 standard deviations
    \emph{above} the mean.
    We know it must be above the mean, because $Z$ is positive.}


Because Z-scores have no units, they are useful for comparing distance to the mean for distributions that have different standard deviations or different units.

\D{\newpage}

\begin{examplewrap}
\begin{nexample}
{The average daily high temperature in June in LA is 77\degree F with a standard deviation of 5\degree{}F.  The average daily high temperature in June in Iceland is 13\degree{}C with a standard deviation of 3\degree{}C.  Which would be considered more unusual:  an 83\degree{}F day in June in LA or a 19\degree{}C day in June in Iceland? }
Both values are 6\degree{} above the mean.  However, they are not the same number of standard deviations above the mean.  83 is $\frac{83-77}{5} = 1.2$ standard deviations above the mean, while 19 is $\frac{19-13}{3} = 2$ standard deviations above the mean.  Therefore, a 19\degree{}C day in June in Iceland would be more unusual than an 83\degree{}F day in June in LA, as the $|Z|$ for 19\degree{}C is greater than for 83\degree{}F ( $|2| > |1.2|$).  
\end{nexample}
\end{examplewrap}






%%

\subsection{Technology: summarizing a single variable}
\label{techsummarizedata}


\noindent Download the \data{loan50} CSV file from \oiRedirect{openintro-data}{openintro.org/data}.  Open it and graphically and numerically summarize the \var{loan$\_$amount} variable.\\

\noindent \textbf{Desmos}:  \oiRedirect{desmos-calculator}{desmos.com/calculator}
\begin{enumerate}
\setlength{\itemsep}{0mm}
\item Open the CSV file.  Copy the column that corresponds to the \var{loan$\_$amount} variable. 
\item  In left panel of Desmos, click in a cell and type:  \calctext{a = } 
\\ \hspace{5mm} Paste the column that you copied after the = sign and then hit return on your keyboard.  You can also manually enter data between the [ ] brackets separated by a comma.
\item Use \calctext{histogram(data, bin width)}, replacing \calctext{data} and \calctext{bin width} with the desired values, as illustrated below.  Here, we named our data \calctext{a}, but you can name it whatever you like.  You can also use \calctext{boxplot(data)} and \calctext{dotplot(data)}. Click the magnifying glass to Zoom Fit the graphing window and click the wrench icon to add labels to the axes.  
\item Use \calctext{stats(data)}, \calctext{mean(data)}, and \calctext{stdev(data)} to numerically summarize the variable as illustrated below.
\begin{center}
\fbox{\Figures [A screenshot from a Desmos calculator showing ``Loan Amount variable" on the left along with its summary statistics.  On the right is a histogram of the data with a box plot below it.  The graphs show that the data are right skewed.]
{.7}{technology1Var}{desmosDescriptive2}}
\end{center}
\end{enumerate}


%%

\D{\newpage}

\noindent In this book we will also provide some basic R code to address the technology questions posed.  In many cases, there will be similarities between the Desmos syntax and the R syntax.  The goal is to provide the interested reader a small glimpse into analyzing data using a real world statistical software language.  These sections can be skipped.  For a more comprehensive introduction to statistical data analysis, we encourage you to explore OpenIntro's Statistical Software labs at \oiRedirect{openintro-labs}{openintro.org/book/statlabs}.  Here you will find labs in R (Base), R (Tidyverse), Rguroo, Jamovi, JASP, Python, SAS, and Stata. \\

\noindent  \R{}:  You will need to first download R and RStudio from: \url{posit.co/download/rstudio-desktop}. 
\begin{enumerate}
\setlength{\itemsep}{0mm}
\item Open the CSV file.  Copy the data that corresponds to the \var{loan$\_$amount} variable.  Do not include the header/variable name.
\item In RStudio, at the \texttt{>} prompt, type: \texttt{a = scan()} and hit return.  Paste the copied data, then hit return and return again.  \\  
\texttt{> \calctext{a = scan()}}\\
\texttt{1: 22000\\
2: 6000\\
\ \ \ \ ... \ \ \ \ \\
49: 20000\\
50: 15000\\
51: \\
Read 50 items}
\label{rDescriptive}
\item Use \texttt{hist()} and \texttt{boxplot()} as shown below.  Replace labels with appropriate values.\\
\texttt{> \calctext{hist(a, main = "Loan Amounts for 50 loans made through Lending Club",\\xlab = "Loan Amount ($\$$)", ylab = "frequency", col = "lightblue")}}\\ \\
\texttt{> \calctext{boxplot(a, main = "Loan Amounts for 50 loans made through Lending Club", xlab = "Loan Amount ($\$$)", horizontal = TRUE, col = "lightblue")}}
\begin{center}
\Figures[add description ]
{0.4}{technology1Var}{rHistogram}\hspace{5mm}
\Figures[add description ]
{0.4}{technology1Var}{rBoxplot}
\end{center}
\item Use \texttt{summary()},\footnote{There are multiple definitions for computing quartiles.  To ensure values match the definition used here, for small data sets of length $n$, you may need to add \texttt{quantile.type=6} when $n$ is odd or \texttt{quantile.type=2} when $n$ is even.} and \texttt{sd()} functions as shown. You can ignore the \texttt{[1]}.\\
\texttt{> \calctext{summary(a)}}\\
\texttt{   
Min. \ \ 1st Qu. \ \ Median\ \ Mean \ \ 3rd Qu.\ \ Max. \\
3000 \ \ \ 7125 \ \ \ \ \ 15500\ \ \ \ 17083\ \ \ 24000\ \ \ 40000 }\\
\texttt{> \calctext{sd(a)}}\\
\texttt{[1] 10455.46}
\end{enumerate}
\noindent Other ways to read in data:\vspace{-1mm}
\begin{itemize}
\item[] Manually enter data between the parentheses of \texttt{c( )} as shown below.\\
\texttt{> \calctext{a = c(22000, 6000, 25000, ..., 5825, 20000, 15000)}}
\item[] Access a CSV file on a computer or on the web.  Adjust path and file name as necessary.\\
\texttt{> \calctext{loan50 = read.csv("Users/FirstnameLastname/Desktop/loan50.csv")}}
%\item[] Access from an \R{} construct link, such as the ones found on \url{openintro.org/data}.\\
%\texttt{> \calctext{source("https://openintro.org/data/R/loan50.R")}}
\item[] Install the \texttt{openintro} package to access \textit{all} data sets found at \oiRedirect{openintro-data}{openintro.org/data}.\\
\texttt{> \calctext{install.packages("openintro")}}\\
\texttt{> \calctext{library(openintro)}}
\end{itemize}
\noindent If using the \texttt{openintro} package or reading from a CSV file, use the structure \texttt{dataset$\$$variable} to access a particular variable.  For example, replace \texttt{a} from above with \var{loan50$\$$loan\_amount}.  \\
\label{rdescriptive}
\texttt{> \calctext{sd(loan50$\$$loan\_amount)}}\\
\texttt{[1] 10455.46}\\  \\
Now explore data sets, for example \texttt{loan50}, with these useful functions: 
\\ \calctext{View(loan50)}, \calctext{str(loan50)}, \calctext{summary(loan50)}, \calctext{head(loan50)}.  \\
\\
\\
\noindent  \textbf{Calculator}:  Your teacher may give you data files.  Alternately, manually enter data into a list as described here.  Instructions for the open-source NumWorks calculator (\oiRedirect{numworks-simulator}{numworks.com/simulator}) are included along with example output.  For the TI-83/84 and Casio calculators, general instructions are provided and worked example videos can be accessed via the \includegraphics[height=3mm]{extraTeX/icons/video_camera.png} icon or at \oiRedirect{ti84_playlist}{\textbf{openintro.org/ti}} and \oiRedirect{casio-9750-playlist}{\textbf{openintro.org/casio}}.\\[3mm]


\begin{onebox}{Numworks: Summarizing data}
Use \calctext{OK} or \calctext{EXE} to make a selection.
\begin{enumerate}
\setlength{\itemsep}{0mm}
\item Click the yellow Home button (above the black Power button) to get to the home screen and select \calctext{Statistics}.
\item Make sure you have \calctext{Data} at the top highlighted, then enter your values.  Use the \calctext{x} button near the upper right to delete an entry.  To clear a whole list, press the up arrow until the list name is highlighted, then press the \calctext{x} button, right arrow, and choose \calctext{Confirm}.
\item Use the arrows to choose \calctext{Graph} and choose the desired graph.  Note that you can use the arrows to get to \calctext{Settings} to change the Bin width and X start for a Histogram.  Using arrows on Boxplot will show each value of the 5-number summary.
\begin{center}
\Figures[ ]
{0.35}{technology1Var}{numworksDescriptive1}\hspace{10mm}
\Figures[add description ]
{0.35}{technology1Var}{numworksDescriptive2}
\end{center}
\item Use the arrows to choose \calctext{Stats} to see the summary statistics.  Use the down arrow to see more summary statistics.  
\begin{center}
\Figures[ ]
{0.35}{technology1Var}{numworksDescriptive3}\hspace{10mm}
\Figures[add description ]
{0.35}{technology1Var}{numworksDescriptive4}
\end{center}
\end{enumerate}
\end{onebox}

\D{\newpage}

%%
\begin{onebox}{\videohref{ti84_entering_data} TI-83/84: Entering data}
The first step in summarizing data or making a graph is to  enter the data set into a list. Use \calcbutton{STAT}, \calctext{Edit}.
\begin{enumerate}
\setlength{\itemsep}{0mm}
\item Press \calcbutton{STAT}.
\item Choose \calctext{1:Edit}.
\item Enter data into \calctext{L1} or another list.
\end{enumerate}
\end{onebox}

\begin{onebox}{\videohref{ti84_calculating_summary_statistics} TI-84: Calculating Summary Statistics}
\label{summstat}
Use the \calcbutton{STAT}, \calctext{CALC}, \calctext{1-Var Stats} command to find summary statistics such as mean, standard deviation, and quartiles.
\begin{enumerate}
\setlength{\itemsep}{0mm}
\item Enter the data as described previously.
\item Press \calcbutton{STAT}.
\item Right arrow to \calctext{CALC}.
\item Choose \calctext{1:1-Var Stats}.
\item Enter \calctext{L1} (i.e. \calcbutton{2ND} \calcbutton{1}) for List. If the data is in a list other than \calctext{L1}, type the name of that list.
\item Leave \calctext{FreqList} blank.
\item Choose \calctext{Calculate} and hit \calcbutton{ENTER}.
\end{enumerate}
TI-83: Do steps 1-4, then type \calctext{L1} (i.e. \calcbutton{2nd} \calcbutton{1}) or the list's name and hit \calcbutton{ENTER}.
\end{onebox}

\noindent Calculating the summary statistics will return the following information. It will be necessary to hit the down arrow to see all of the summary statistics.

\begin{center}
\begin{tabular}{ll l ll}
$\calctextmath{\bar{\text{x}}}$ & Mean &\quad&
	\calctext{n} & Sample size or \# of data points \\
$\calctextmath{\Sigma x}$ & Sum of all the data values &&
	\calctext{minX} & Minimum \\
$\calctextmath{\Sigma x^2}$ & Sum of all the squared data values &&
	$\calctextmath{Q_1}$ & First quartile\\
$\calctextmath{Sx}$ & Sample standard deviation &&
	\calctext{Med} & Median \\
$\calctextmath{\sigma x}$ & Population standard deviation &&
	\calctext{maxX} & Maximum 
\end{tabular}
\end{center}

\begin{onebox}{\videohref{ti84_box_plot} TI-83/84:  Drawing a box plot}
\label{boxplot}
\ \vspace{-5mm} \
\begin{enumerate}
\setlength{\itemsep}{0mm}
\item Enter the data to be graphed as described previously.
\item Hit \calcbutton{2ND} \calcbutton{Y=} (i.e. \calctext{STAT PLOT}).
\item Hit \calcbutton{ENTER} (to choose the first plot).
\item Hit \calcbutton{ENTER} to choose \calctext{ON}.
\item Down arrow and then right arrow three times to select box plot with outliers.
\item Down arrow again and make \calctext{Xlist:}~\calctext{L1} and \calctext{Freq:}~\calctext{1}.
\item Choose \calctext{ZOOM} and then \calctext{9:ZoomStat} to get a good viewing window.
\end{enumerate}
\end{onebox}

\begin{onebox}{TI-83/84: What to do if you cannot find {L1} or another list}
Restore lists \calctext{L1}-\calctext{L6} using the following steps:
\begin{enumerate}
\setlength{\itemsep}{0mm}
\item Press \calcbutton{STAT}.
\item Choose \calctext{5:SetUpEditor}.
\item Hit \calcbutton{ENTER}.
\end{enumerate}\end{onebox}

%%
\D{\newpage}

\begin{onebox}{\videohref{casio_1_var_stats_and_box_plot} Casio fx-9750GII: Entering data}
\ \vspace{-5mm} \
\begin{enumerate}
\setlength{\itemsep}{0mm}
\item Navigate to \calctext{STAT} (\calcbutton{MENU} button, then hit the \calcbutton{2} button or select \calctext{STAT}).
\item Optional: use the left or right arrows to select a particular list.
\item Enter each numerical value and hit \calcbutton{EXE}.
\end{enumerate}
\end{onebox}

\begin{onebox}{\videohref{casio_1_var_stats_and_box_plot} Casio fx-9750GII: Drawing a box plot and 1-variable statistics}
\ \vspace{-5mm} \
\begin{enumerate}
\setlength{\itemsep}{0mm}
\item Navigate to \calctext{STAT} (\calcbutton{MENU}, then hit \calcbutton{2}) and enter the data into a list.
\item Go to \calctext{GRPH} (\calcbutton{F1}).
\item Next go to \calctext{SET} (\calcbutton{F6}) to set the graphing parameters.
\item To use the 2nd or 3rd graph instead of \calctext{GPH1}, select \calcbutton{F2} or \calcbutton{F3}.
\item Move down to \calctext{Graph Type} and select the $\calctextmath{\triangleright}$ (\calcbutton{F6}) option to see more graphing options, then select \calctext{Box} (\calcbutton{F2}).
\item If \calctext{XList} does not show the list where you entered the data, hit \calctext{LIST} (\calcbutton{F1}) and enter the correct list number.
\item Leave \calctext{Frequency} at \calctext{1}.
\item For \calctext{Outliers}, choose \calctext{On} (\calcbutton{F1}).
\item Hit \calcbutton{EXE} and then choose the graph where you set the parameters \calcbutton{F1} (most common), \calcbutton{F2}, or \calcbutton{F3}.
\item If desired, explore 1-variable statistics by selecting \calctext{1-Var} (\calcbutton{F1}).
\end{enumerate}
\end{onebox}

\begin{onebox}{\videohref{casio_1_var_stats_and_box_plot} Casio fx-9750GII: Deleting a data list}
\ \vspace{-5mm} \
\begin{enumerate}
\setlength{\itemsep}{0mm}
\item Navigate to \calctext{STAT} (\calcbutton{MENU}, then hit \calcbutton{2}).
\item Use the arrow buttons to navigate to the list you would like to delete.
\item Select $\calctextmath{\triangleright}$ (\calcbutton{F6}) to see more options.
\item Select \calctext{DEL-A} (\calcbutton{F4}) and then \calcbutton{F1} to confirm.
\end{enumerate}
\end{onebox}

\D{\newpage}

%%
\subsection*{Section summary}

\begin{itemize}
 \item The \termni{mean} and the \termni{median} are measures of \termni{center}.  

\begin{itemize}
\item   The \termni{mean} is the sum of all the observations divided by the
  number of observations, $n$. \\
  $\bar{x} = \frac{1}{n}\sum{x_{i}} = \frac{\sum{x_i}}{n}=\frac{x_1 + x_2 + ... + x_n}{n}$\\
  
\item In an ordered data set, the \termni{median} is the middle number when $n$ is odd.  When $n$ is even, the median is the average of the two middle numbers. 

\end{itemize} 

\item The mean follows the tail.  In a \termni{right skewed} distribution, the mean is greater than the median.  Analogously, in a \termni{left skewed} distribution, the mean is less than the median.  


\item The pth \termni{percentile} of a data set is the value that has p\% of the data less than or equal to it when the data set is ordered from smallest to largest. The first quartile ($Q_1$) is the 25th percentile, the median ($Q_2$) is the 50th percentile, and the third quartile ($Q_3$) is the 75th percentile.  

\item \termni{Standard deviation (SD)}and \termni{Interquartile range (IQR)} are measures of spread.  SD measures the typical spread from the mean, whereas IQR measures the spread of the middle 50\% of the data. \termni{Range} is also sometimes used as a measure of spread. 
\begin{itemize}
\item Standard deviation is the square root of the variance.  $s_{\scriptscriptstyle{X}} = \sqrt{\frac{1}{n-1} \sum{(x_i -  \bar{x})^2}}$ 

\item IQR = $Q_3 - Q_1$, i.e. the difference between the third quartile and the first quartile.

\item Range = $max - min$, i.e. the difference between the maximum value and minimum value.

\end{itemize}


\item \termni{Changing units}.  Adding a constant to every value in a data set shifts the mean but does not affect the standard deviation. Multiplying the values in a data set by a constant  multiplies the mean and the standard deviation by that constant, except that the standard deviation must always remain positive. 

\item \termsub{Outliers}{outlier} are observations that are extreme relative to the rest of the data.  Two rules of thumb for identifying observations as outliers are:
\begin{itemize}\vspace{-1mm}
\setlength{\itemsep}{0mm}
\item more than 2 standard deviations above or below the mean
\item more than $1.5 \times IQR$ below $Q_1$ or above $Q_3$
\end{itemize}

\item Mean, standard deviation, and range are sensitive to outliers and are considered nonresistant (non-robust) measures.  Median and IQR are more robust and less sensitive to outliers.

\item Summary statistics of a quantitative variable may reveal information that can
be used to justify claims about the variable in context

\item \termni{Box plots} provide a graphical representation of the \termni{five-number summary}, which consists of: $min$, $Q_1$, $Q_2$, $Q_3$, $max$.  While a box plot does not indicate frequency or modes, it can show skew and outliers.  

\item Histograms and dot plots on the same scale, back-to-back stem-and-leaf plots, and parallel box plots can be used to compare important features between two or more distributions of the same numerical variable.

\item A comparison of graphical representations for two or more distributions should
include a comparison of center, spread, shape, outliers, and any unusual features.  Put descriptions in \textit{context}, identifying the variable(s) being summarized by name and including relevant units.

\item Multiple quantitative one-variable graphical representations may reveal
information that can be used to justify claims about the variable in context.

\item A \termni{Z-score} represents the number of standard deviations a value in a data set is above or below the mean.  To calculate a \mbox{Z-score} use: $Z = \frac{x-\text{mean}}{SD}$.  \emph{Z-scores do not depend on units} and are useful for comparing relative positions of individual values within a distribution or between distributions

\end{itemize}


%%%%%%%%%%%Section Exercises
{\input{ch_one_variable_data_collecting_data/TeX/numerical_summaries_and_box_plots.tex}}



%_______________________________
\section[Overview of data collection principles]{Overview of data collection principles }
\label{overviewOfDataCollectionPrinciples}

\index{sample|(}
\index{population|(}
\index{parameter|(}
\index{statistic|(}

\sectionintro{
\noindent%
How do researchers collect data?  Why are the results of some studies more reliable than others?
The way a researcher collects data depends upon the research goals.
In~this section, we look at different methods of collecting data and consider the types of conclusions that can be drawn from those methods.
%%
\subsection*{Learning objectives}

\begin{enumerate}
\setlength{\itemsep}{0mm}

\item Identify the main types of data collection: census, experiment, and observation study, and determine which one is appropriate for a given investigative question.
 
\item Classify a study as observational or experimental, and determine when a study's results can be generalized to the population and when a causal relationship can be drawn.

\item Identify and explain possible confounding factors within a study.

\end{enumerate}
}

%%
\subsection{The investigative question and types of conclusions}
In Section~\ref{dataBasics}, we saw that the first step of the investigative process is to identify a research question.  An investigative question for a specific study should have a defined purpose.  There are two types of conclusions that can be drawn from statistical studies.  The first is a \emph{causal conclusion} about the effect of a treatment and the second is a \emph{generalization} or inference from a sample to a larger population.  

If the purpose of the study is to draw a causal conclusion, an experiment with random assignment of treatments should be implemented.  For example, if researchers want to see if a certain drug \emph{causes} a reduction of blood pressure, they would carry out an experiment and randomly assign some people to take the drug and others to take a placebo (fake treatment).  

If, on the other hand, the purpose is to generalize the results of a sample to a larger population, an observational study with random sampling should be used.  For example, researchers often want to estimate a parameter or quantity about a population, such as the average household size or proportion of registered voters that will vote for a certain candidate for mayor.  Here, it is not an experiment that is needed, but rather a random sample from the target population, in this case registered voters in the city.  Taking a random sample from the population will allow the researcher to generalize and say that the estimate from the sample (the statistic) is a reasonable estimate for the population parameter.

Sometimes we want to draw a causal conclusion \emph{and} generalize that causal conclusion to a larger population.  To be able to draw both types of conclusions, we need a random sample of individuals and then a randomized experiment implemented on those individuals.  Here we contrasted the primary goal of a random sample with the primary goal of an experiment.  We explore sampling techniques and experimental design more thoroughly later in this chapter.

The second step of the investigative process is data collection.  The nature of the research question will determine whether an experiment or observational study is appropriate.  The third step of the statistical process is analysis.  A researcher should have a well-define variable of interest and a clearly stated parameter of interest.  We will consider two main types of analysis in this textbook:  confidence intervals and hypothesis tests, both of which will be introduced in Chapter~\ref{ch_inference_for_props}.  With confidence intervals, we seek to estimate the parameter within a range of reasonable values.  With hypothesis tests, we seek to determine how much evidence there is that the parameter is greater than, less than, or different from a certain hypothesized value. 

\D{\newpage}

Lastly, it is important that the investigative question indicates the type(s) of conclusion(s) applicable from the study. The investigative question should provide the population to which the conclusions will be applicable and, in the case of an experiment that uses random assignment, a cause-and-effect conclusion.



%%
\subsection{Anecdotal evidence}
\label{anecdotalEvidenceSubsection}

Consider the following possible responses to the three research questions:
\begin{enumerate}
\item A man on the news got mercury poisoning from eating swordfish, so the average mercury concentration in swordfish must be dangerously high.
\item\label{iKnowThreeStudentsWhoTookMoreThan7YearsToGraduateAtDuke} I met two students who took more than 7~years to graduate from Duke, so it must take longer to graduate at Duke than at many other colleges.
\item\label{myFriendsDadDiedAfterSulphinpyrazon} My friend's dad had a heart attack and died after they gave him a new heart disease drug, so the drug must not work.
\end{enumerate}
Each conclusion is based on data. However, there are two problems. First, the data only represent one or two cases. Second, and more importantly, it is unclear whether these cases are actually representative of the population. Data collected in this haphazard fashion are called \term{anecdotal evidence}.

\captionsetup{width=\textwidth-80mm}
\begin{figure}
\centering
  \hspace{8mm}\Figuress
    [A wintery scene, where the trees are covered in snow, and there are large piles of snow on the sides of the roads. This particular photo was taken at the University of Minnesota campus following a storm after which tree branches were a particularly vibrant white color after the storm.]
    {55mm}{mnWinter}{mnWinter}\hspace{4mm}
  \begin{minipage}[b]{\textwidth-75mm}
   \caption[anecdotal evidence]{In February 2010, some media pundits cited one large snow storm as valid evidence against global warming. As comedian Jon Stewart pointed out, ``It's one storm, in one region, of one country.''\vspace{-4.5mm} \\

   -----------------------------\vspace{-2mm}\\
   {\footnotesize February 10th, 2010.}
   \label{mnWinter}}
\end{minipage}
\end{figure}
\captionsetup{width=\mycaptionwidth}

\begin{onebox}{Anecdotal evidence}
Be careful of making inferences based on anecdotal evidence. Such evidence may be true and verifiable, but it may only represent extraordinary cases. The majority of cases and the average case may in fact be very different.\end{onebox}

Anecdotal evidence typically is composed of unusual cases that we recall based on their striking characteristics. For instance, we may vividly remember the time when our friend bought a lottery ticket and won \$250 but forget most the times she bought one and lost. Instead of focusing on the most unusual cases, we should examine a representative sample of many cases.


\D{\newpage}

%%
\subsection{Explanatory and response variables}
\label{explanatoryAndResponse}


When we ask questions about the relationship
between two variables, we sometimes also want to determine
if the change in one variable causes a change in the other.
Consider the following rephrasing of an earlier question
about the \data{county} data set:
\begin{quote}\em
  If there is an increase in the median household income
  in a county, does this drive an increase in its population?
\end{quote}
In this question, we are asking whether one variable
affects another.
If this is our underlying belief,
then \emph{median household income} is the
\termsub{explanatory}{explanatory variable}
variable and the \emph{population change} is the
\termsub{response}{response variable} variable
in the hypothesized relationship.\footnote{Sometimes
  the explanatory variable is called the \term{independent}
  variable and the response variable is called the
  \term{dependent} variable.
  However, this becomes confusing since a \emph{pair}
  of variables might be independent or dependent,
  so we avoid this language.}


\begin{onebox}{Explanatory and response variables}
When we suspect one variable might causally affect another,
we label the first variable the explanatory variable and the
second the response variable.
\vspace{1mm}

\hspace{10mm}\Figure
    [Simple graphic shown the words "explanatory variable" pointing to "response variable", where the words "might affect" appear above the arrow.]
    {0.53}{expResp}

For many pairs of variables, there is no hypothesized
relationship, and these labels would not be applied to
either variable in such cases.
\end{onebox}

\begin{onebox}{Association does not imply causation}
{Labeling variables as \emph{explanatory} and \emph{response} does not guarantee the relationship between the two is actually causal, even if there is an association identified between the two variables. We use these labels only to keep track of which variable we suspect affects the other.}
\end{onebox}

In many cases, the relationship is complex or unknown. It may be unclear whether variable $A$ explains variable $B$ or whether variable $B$ explains variable $A$. For example, it is now known that a particular protein called REST is much depleted in people suffering from Alzheimer's disease. While this raises hopes of a possible approach for treating Alzheimer's, it is still unknown whether the lack of the protein causes brain deterioration, whether brain deterioration causes depletion in the REST protein, or whether some third variable causes both brain deterioration and REST depletion. That~is, we do not know if the lack of the protein is an explanatory variable or a response variable. Perhaps it is both.\footnote{\oiRedirect{textbook-nytimes_gene_alzheimers_study}{nytimes.com/2014/03/20/health/fetal-gene-may-protect-brain-from-alzheimers-study-finds.html}}


\D{\newpage}

%%
\subsection{Introduction to experiments}
\noindent There are two primary types of data collection: experiments and observational studies.

When researchers want to investigate the possibility of a causal connection, they conduct an \term{experiment}. For all experiments, the researchers must impose a treatment. For most studies there will be both an explanatory and a response variable. For instance, we may suspect administering a drug will reduce mortality in heart attack patients over the following year. To check if there really is a causal connection between the explanatory variable and the response, researchers will collect a sample of individuals and split them into groups.  The individuals in each group are \emph{assigned} a treatment. When individuals are randomly assigned to a group, the experiment is called a \term{randomized experiment}. For example, each heart attack patient in a drug trial could be randomly assigned into one of two groups: the first group receives a \termni{placebo} (fake treatment) and the second group receives the drug. See the case study in Section~\ref{basicExampleOfStentsAndStrokes} for another example of an experiment.

In an experiment, a treatment is applied to an observational unit called an \term{experimental unit}.  When the experimental units are people, we commonly refer to them as subjects or participants rather than experimental units.  In our example, each heart attack patient received a placebo or a drug, so the experimental unit or subject in this study is a heart attack patient.  At the end of the study, the \term{response variable} is measured on each experimental unit.  Here, there response variable could be whether or not a heart attack patient survived at least 5 years after receiving the drug or placebo.  Reachers will compare the proportion in each group that survived at least 5 years.  If the difference is \emph{significant}, and if the experiment is well-designed, then the researchers will be able to draw a causal conclusion about the effect of the drug.  

Some experiments study more than one factor (explanatory variable) at a time, and each of these factors may have two or more levels (possible values). For example, suppose a researcher plans to investigate how the type and volume of music affect a person's performance on a particular video game. Because these two factors, \var{type} and \var{volume}, could interact in interesting ways, we do not want to test one factor at a time. %
%Instead, we want to do a \term{factorial experiment} in which 
Instead, we want to do an experiment in which
we test all \emph{combinations} of the factors. Let's say that \var{volume} has two levels (soft and loud) and that \var{type} has three levels (dance, classical, and punk). Then, we would want to have experiment groups for each of the six (2 x 3 = 6) combinations: soft dance, soft classical, soft punk, loud dance, loud classical, loud punk. Each combination is a \term{treatment}. Therefore, this experiment will have 2 factors and 6~treatments. To replicate each treatment 10 times, one would need to play the game 60~times.

\begin{exercisewrap}
\begin{nexercise}A researcher wants to compare the effectiveness of four different drugs. She also wants to test each of the drugs at two doses: low and high. Describe the factors, levels, and treatments of this experiment.\footnotemark
\end{nexercise}
\end{exercisewrap}
\footnotetext{There are two factors: type of drug, which has four levels, and dose, which has 2 levels. There will be 4 x 2 = 8 treatments: drug 1 at low dose, drug 1 at high dose, drug 2 at low dose, and so on.}

As the number of factors and levels increases, the number of treatments become large and the analysis of the resulting data becomes more complex, requiring the use of advanced statistical methods. We will investigate only one factor at a time in this book.


\D{\newpage}

\subsection{Observational studies}

Researchers perform an \term{observational study} when they collect data without interfering with how the data arise. For instance, researchers may collect information via surveys, review medical or company records, or follow a \term{cohort} of many similar individuals to study why certain diseases might develop. In each of these situations, researchers merely observe or take measurements of things that arise naturally.

Observational studies come in two forms: prospective and retrospective studies.
A \term{prospective study} identifies individuals and collects information as
events unfold.
For instance, medical researchers may identify and follow a group of similar
individuals over many years to assess the possible influences of behavior on
cancer risk.
One example of such a study is The Nurses' Health Study, started in 1976
and expanded in 1989.
This prospective study recruits registered nurses and then collects data from
them using questionnaires.
\termsub{Retrospective studies}{retrospective studies} collect data after
events have taken place,
e.g. researchers may review past events in medical records.
Some data sets, such as \data{county}, may contain both prospectively-
and retrospectively-collected variables.
Local governments prospectively collect some variables as events unfolded
(e.g. retails sales) while the federal government retrospectively collected
others during the 2010 census (e.g. county population counts).



\begin{examplewrap}
\begin{nexample}{Suppose that a researcher is interested in the average tip customers at a particular restaurant give. Should she carry out an observational study or an experiment?}
In addressing this question, we ask, ``Will the researcher be imposing any treatment?"  Because there is no treatment or interference that would be applicable here, it will be an observational study. Additionally, one consideration the researcher should be aware of is that, if customers know their tips are being recorded, it could change their behavior, making the results of the study inaccurate.
\end{nexample}
\end{examplewrap}


Making causal conclusions based on experiments is often reasonable. However, making the same causal conclusions based on observational data is treacherous and is not recommended. Observational studies are generally only sufficient to show associations.

\begin{exercisewrap}
\begin{nexercise} \label{sunscreenConfoundingExample}
Suppose an observational study tracked sunscreen use and skin cancer, and it was found people who use sunscreen are more likely to get skin cancer than people who do not use sunscreen. Does this mean sunscreen \emph{causes} skin cancer?\footnotemark
\end{nexercise}
\end{exercisewrap}
\footnotetext{No. See the paragraph following the exercise for an explanation.}

Some previous research tells us that using sunscreen actually reduces skin cancer risk, so maybe there is another variable that can explain this hypothetical association between sunscreen usage and skin cancer. One important piece of information that is absent is sun exposure. Sun exposure is what is called a \term{confounding variable}, \term{confounding factor}, or a \term{confounder}).

\begin{center}
  \Figures
    [There are three boxes with words positioned in a triangle. One box has "sun exposure" written in it, and that box has two arrows pointing from it to the two other boxes, which are labeled "use sunscreen" and "skin cancer". There is a third arrow more lightly colored and pointing from the "use sunscreen" box to the "skin cancer" box, where a question mark has been placed above that lightly-colored arrow.]
    {0.55}{variables}{sunCausesCancer}
\end{center}
% Some studies:
% http://www.sciencedirect.com/science/article/pii/S0140673698121682
% http://archderm.ama-assn.org/cgi/content/abstract/122/5/537
% Study with a similar scenario to that described here:
% http://onlinelibrary.wiley.com/doi/10.1002/ijc.22745/full



\begin{onebox}{Confounding variable}
A confounding variable is a variable that is associated with both the explanatory \emph{and} response variables. Because of the confounding variable's association with both variables, we do not know if the response is due to the explanatory variable or due to the confounding variable.\end{onebox}

\D{\newpage}

Sun exposure is a confounding factor because it is associated with both the use of sunscreen and the development of skin cancer. People who are out in the sun all day are more likely to use sunscreen, and people who are out in the sun all day are more likely to get skin cancer. Research shows us the development of skin cancer is due to the sun exposure. The variables of sunscreen usage and sun exposure are \term{confounded}, and without this research, we would have no way of knowing which one was the true cause of skin cancer.

\begin{examplewrap}
\begin{nexample}{In a study that followed 1,169 non-diabetic adults who had been hospitalized for a first heart attack, the people that reported eating chocolate had increased survival rate over the next 8 years than those that reported not eating chocolate. Also, those who ate more chocolate tended to live longer on average. The researchers controlled for several confounding factors, such as age, physical activity, smoking, and many other factors. Can we conclude that the consumption of chocolate caused the people to live longer?} \label{confounding_2008_chocolate_health_study}
This is an observational study, not a controlled randomized experiment. Even though the researchers controlled for many possible variables, there may still be other confounding factors. (Can you think of any that weren't mentioned?) While it is possible that the chocolate had an effect, this study cannot prove that chocolate increased the survival rate of patients.
\end{nexample}
\end{examplewrap}

\begin{examplewrap}
\begin{nexample}{The authors who conducted the study did warn in the article that additional studies would be necessary to determine whether the correlation between chocolate consumption and survival translates to any causal relationship. That is, they acknowledged that there may be confounding factors. One possible confounding factor not considered was mental health. In context, explain what it would mean for mental health to be a confounding factor in this study.}
Mental health would be a confounding factor if, for example, people with better mental health tended to eat more chocolate, and those with better mental health \emph{also} were less likely to die within the 8 year study period. Notice that if better mental health were not associated with eating more chocolate, it would not be considered a confounding factor because it wouldn't explain the observed associated between eating chocolate and having a better survival rate. If better mental health were associated only with eating chocolate and not with a better survival rate, then it would also not be confounding for the same reason. Only if a variable that is associated with both the explanatory variable of interest (chocolate) and the outcome variable in the study (survival during the 8 year study period) can it be considered a confounding factor.
\end{nexample}
\end{examplewrap}

While one method to justify making causal conclusions from observational studies is to exhaust the search for confounding variables, there is no guarantee that all confounding variables can be examined or measured.



\begin{onebox}{Association $\neq$ causation}
In general, association does not imply causation, and causation can only be inferred from a randomized experiment.
\end{onebox}


\D{\newpage}

%%
\subsection*{Section summary}
\begin{itemize}
\setlength{\itemsep}{0.5mm}

\item The nature and purpose of the investigative question should guide the data
collection process and should be phrased in terms of the variable(s) of interest
in the study.  If the purpose is to draw a causal conclusion an experiment should be used.  If the purpose is to make a generalization from a sample to a larger population, a random sample from the population of interest should be taken.

\item Depending on the investigative question, one of two main data analysis methods may be used:  confidence intervals or hypothesis tests.  With confidence intervals, we seek to estimate a stated parameter within a range of reasonable values.  With hypothesis tests, we seek to determine how much evidence there is that the stated parameter is greater than, less than, or different from a certain hypothesized value. 

\item An investigative question should indicate the type(s) of conclusion(s) applicable from the study. The investigative question should
provide the population to which the conclusions will be applicable and, in the case of an experiment that uses random assignment, a cause-and-effect
conclusion.

\item A \termni{census} consists of recording information from all items or individuals in a population.

\item An \termni{experiment} is a study in which a researcher assigns conditions, or treatments, to experimental units to explore an investigative question of
interest.  The goal is to draw a \emph{causal} conclusion about the effect of the treatment(s).

\item The experimental unit is the observational unit to which the treatment is assigned. When experimental units consist of people, they are sometimes referred to as subjects or participants.

\item In an experiment, an \termni{explanatory variable}, or factor, is a variable whose different categories,
or levels, are imposed on the experimental units. The different categories, or
levels, of the explanatory variable are called treatments. When there is more
than one explanatory variable, the combinations of the categories, or levels, of
the explanatory variables are called treatments.

\item A \termni{response variable} is an outcome measured on each experimental unit after
the treatment has been administered.

\item An \termni{observational study} is a study where treatments are not imposed. The
researcher records the values of the variables of interest in order to explore an
investigative question of interest.  

\item To be able to genera

\item A \termni{prospective study} is one in which the observational units of study are
selected at a point in time, and data are gathered both at that time and into the
future.

\item A \termni{retrospective study} is one in which the observational units of study are
selected at a point in time and data from the past are gathered.

\item A \termni{survey} is an observational study in which the data are collected from humans
using a standard set of questions.

\item A \termni{confounding variable} in an observational study provides an alternative
explanation for the observed relationship between the explanatory and
response variables determined in the study, thereby lowering the credibility of
the assertion of a causal relationship between the explanatory and response
variables of interest. To be a confounding variable, a variable must be
associated with both the explanatory variable and the response variable.

\item A sample is considered random when all observational units in the sample have
an equal chance of being selected from the population. A random mechanism
is any resource used to select the observational units to be included in the
sample.

\item  It is appropriate to generalize from a sample to the population of individuals from which the sample was selected only when the individuals in the sample are randomly
selected from the population.  Do not generalize from anecdotal evidence or from volunteer or other types of non-random samples.

\item When observational units, or experimental units, in a sample are not randomly
selected from a population, it is appropriate to make generalizations only about
a population of individuals that are similar to those used in the study.
  
\end{itemize}

%%%section exercises
{\input{ch_one_variable_data_collecting_data/TeX/overview_of_data_collection_principles.tex}}



%_____________________________________

\section[Sampling methods and sources of bias]{Sampling methods and sources of bias}
\label{section_obs_data_sampling}

\sectionintro{
\noindent%
How do opinion polls work?
    How do research organizations collect the data,
    and what types of bias should we look out for?  You have probably read or heard claims from many studies and polls. 
A~background in statistical reasoning will help you assess the validity of such claims.   


%%
\subsection*{Learning objectives}

\begin{enumerate}
\setlength{\itemsep}{0mm}
  
\item Identify and describe how to implement different random sampling methods, including simple, stratified, cluster, and systematic.  

\item Justify the appropriateness of a sampling method.
 
\item Identify potential sources of bias in sampling methods.
 
\item Understand when it is valid to generalize and to what population that generalization can be made.  
\end{enumerate}

}


%%

%%
\subsection{Sources of bias when sampling from a population}

\index{sample!random sample|(}

We might try to estimate the time to graduation for Duke undergraduates in the last 5 years by collecting a sample of students. All graduates in the last 5 years represent the \emph{population}\index{population}, and graduates who are selected for review are collectively called the \emph{sample}\index{sample}. The goal is to use information from the sample to generalize or make an inference to the population.  In order to be able to generalize, we must \emph{randomly} select a sample from the population of interest. The most basic type of random selection is equivalent to how raffles are conducted. For example, in selecting graduates, we could write each graduate's name on a raffle ticket and draw 100 tickets. The selected names would represent a random sample of 100 graduates.

\begin{figure}[ht]
\centering
  \Figures
    [Graphic showing a larger circle on the left for "all graduate" and a smaller circle on the right for "sample". There are a large number of dots randomly scattered around inside the left circle, and five of those dots have arrows originating from them and pointing to 5 dots inside the right circle. Besides those 5 dots, there are no other dots in the right circle.]
    {0.5}{popToSample}{popToSampleGraduates}
\caption{In this graphic, five graduates are randomly selected from the population to be included in the sample.}
\label{popToSampleGraduates}
\end{figure}

Why pick a sample randomly? Why not just pick a sample by hand? Consider the following scenario.

\begin{examplewrap}
\begin{nexample}{Suppose we ask a student who happens to be majoring in nutrition to select several graduates for the study. What kind of students do you think she might select? Do you think her sample would be representative of all graduates?}
Perhaps she would pick a disproportionate number of graduates from health-related fields. Or perhaps her selection would be well-representative of the population. When selecting samples by hand, we run the risk of picking a \emph{biased} sample, even if that bias is unintentional or difficult to discern.
\end{nexample}
\end{examplewrap}

\begin{figure}
\centering
\includegraphics[width=0.47\textwidth]{ch_one_variable_data_collecting_data/figures/popToSample/popToSubSampleGraduates}
\caption{Instead of sampling from all graduates equally, a nutrition major might inadvertently pick graduates with health-related majors disproportionately often.}
\label{popToSubSampleGraduates}
\end{figure}

\termsub{Bias}{bias} in a sampling method is a systematic error in the sampling procedure that
results in a statistic being consistently larger or consistently smaller than the
parameter the statistic is used to estimate.  There are multiple types of bias when sampling.  If the student majoring in nutrition picked a disproportionate number of graduates from health-related fields, this would introduce undercoverage bias into the sample. \termsub{Undercoverage bias}{undercoverage bias} occurs when some individuals of the population are inherently less likely to be included in the sample than others, making the sample not representative of the population.  In the example, this bias creates a problem because a degree in health-related fields might take more or less time to complete than a degree in other fields. Suppose that it takes longer. Since graduates from other fields would be less likely to be in the sample, the undercoverage bias would cause her to \emph{overestimate} the parameter.

Sampling randomly resolves the problem of undercoverage bias, \emph{if the sample is randomly selected from the entire population of interest}. If the sample is randomly selected from only a subset of the population, say, only graduates from health-related fields, then the sample will not be representative of the population of interest.  Generalizations can only be made to the population from which the sample is randomly selected.

Another common downfall is a \term{convenience sample}\index{sample!convenience sample}, where individuals who are easily accessible are more likely to be included in the sample. For instance, if a political survey is done by stopping people walking in the Bronx, this will not represent all of New York City. It is often difficult to discern what sub-population a convenience sample represents.

\begin{figure}[h]
\centering
  \Figures
    [Graphic showing a larger circle on the left for "population of interest" and a smaller circle on the right for "sample". There are a large number of dots randomly scattered around inside the left circle. A smaller circle annotated as "population actually sampled" is inside this circle and contains a subset of those dots, among which five have arrows originating from them and pointing to 5 dots inside the right circle. Besides those 5 dots, there are no other dots in the right circle.]
    {0.5}{popToSample}{surveySample}
\caption{Due to the possibility of non-response, surveys studies may only reach a certain group within the population. It is difficult, and often times impossible, to completely fix this problem.}
\label{surveySample}
\end{figure}

Similarly, a volunteer sample is one in which people's responses are solicited and those who choose to participate, respond. This introduces \term{voluntary response bias}, which is a problem because those who choose to participate may tend to have different opinions than the rest of the population, resulting in a biased sample.\D{\vspace{-1mm}}

\begin{exercisewrap}
\begin{nexercise}
We can easily access ratings for products, sellers, and companies through websites. These ratings are based only on those people who go out of their way to provide a rating. If 50\% of online reviews for a product are negative, do you think this means that 50\% of buyers are dissatisfied with the product?\footnotemark\D{\vspace{-1mm}}
\end{nexercise}
\end{exercisewrap}
\footnotetext{Answers will vary. From our own anecdotal experiences, we believe people tend to rant more about products that fell below expectations than rave about those that perform as expected. For this reason, we suspect there is a negative bias in product ratings on sites like Amazon. However, because our experiences may not be representative, we also keep an open mind.}\D{\vspace{-1mm}}

The act of taking a random sample helps minimize bias; however, bias can crop up in other ways. Even when people are picked at random, e.g. for surveys, caution must be exercised if the \term{non-response} \index{sample!non-response|textbf} is high. For instance, if only 30\% of the people randomly sampled for a survey actually respond, then it is unclear whether the results are \term{representative} of the entire population. This \term{non-response bias} \index{sample!non-response bias|textbf} can skew results.

Even if a sample has no undercoverage bias and no non-response bias, there is an additional type of bias that often crops up and undermines the validity of results, known as response bias. \termsub{Response bias}{response bias} refers to a broad range of factors that influence how a person responds, such as question wording, question order, and influence of the interviewer. This type of bias can be present even when we collect data from an entire population in what is called a \term{census}. Because response bias is often subtle, one must pay careful attention to how questions were asked when attempting to draw conclusions from the data.\D{\vspace{-1mm}}

\begin{examplewrap}
\begin{nexample}{Suppose a high school student wants to investigate the student body's opinions on the food in the cafeteria. Let's assume that she manages to survey every student in the school. How might response bias arise in this context?\D{\vspace{-1mm}}}
There are many possible correct answers to this question. For example, students might respond differently depending upon who asks the question, such as a school friend or someone who works in the cafeteria. The wording of the question could introduce response bias. Students would likely respond differently if asked ``Do you like the food in the cafeteria?" versus ``The food in the cafeteria is pretty bad, don't you think?"\D{\vspace{-1mm}}
\end{nexample}
\end{examplewrap}

\begin{onebox}{Watch out for bias}
Undercoverage bias, non-response bias, and response bias can still exist within a random sample. Always determine how a sample was chosen, ask what proportion of people failed to respond, and critically examine the wording of the questions.
\end{onebox}\D{\vspace{-1mm}}

When there is no bias in a sample, increasing the sample size tends to increase the precision and reliability of the estimate. When a sample is biased, it may be impossible to decipher helpful information from the data, even if the sample is very large.\D{\vspace{-1mm}}

\begin{exercisewrap}
\begin{nexercise}
A researcher sends out questionnaires to 50 randomly selected households in a particular town asking whether or not they support the addition of a traffic light in their neighborhood. Because only 20\% of the questionnaires are returned, she decides to mail questionnaires to 50 more randomly selected households in the same neighborhood. Comment on the usefulness of this approach.\footnotemark\D{\vspace{-1mm}}
\end{nexercise}
\end{exercisewrap}
\footnotetext{The researcher should be concerned about non-response bias, and sampling more people will not eliminate this issue. The same type of people that did not respond to the first survey are likely not going to respond to the second survey. Instead, she should make an effort to reach out to the households from the original sample that did not respond and solicit their feedback, possibly by going door-to-door.}

\index{sample!random sample|)}
\index{population|)}
\index{sample|)}




%%
\subsection[Simple, systematic, stratified, and cluster sampling]{Simple, systematic, stratified, and cluster sampling}
\label{threeSamplingMethods}

Almost all statistical methods for observational data rely on a sample being random and unbiased. When a sample is collected using a nonrandom sampling method, such as a convenience or voluntary response sample, potential bias is introduced and a generalization to the population of interest is not warranted.  

The most basic random sample is called a \term{simple random sample}, which is equivalent to using a raffle to select cases. This means that each case in the population has an equal chance of being included and there is no implied connection between the cases in the sample.   Three other common random sampling methods include: systematic, stratified, and cluster.  Figure~\ref{simple_systematic} provides a graphical representation of simple versus systematic sampling while Figure~\ref{stratified_cluster} provides a graphical representation of stratified and cluster sampling.

\begin{figure}[p]
\centering
\Figures [Two figures are shown, one positioned above the other. The first is a large rectangle containing many points, where 18 random points are circled and are a different color than the other points, illustrating a simple random sample. The lower figure is also a large rectangle containing many points, but in this rectangle, a random starting point is chosen and every 7th point is circled and a different color, illustrating a systematic random sample.]{0.93}{samplingMethodsFigure}{simple_systematic}
\caption{Examples of simple random sampling and systematic sampling. In the top panel, simple random sampling was used to randomly select 18 cases. In the lower panel, systematic random sampling was used to select every 7th individual.}
\label{simple_systematic}
\end{figure}

\termsub{Simple random sampling}{sample!simple random sampling} is probably the most intuitive form of random sampling. Consider the salaries of Major League Baseball (MLB) players, where each player is a member of one of the league's 30 teams.  We will use N to represent the population size.  Here N is the total number of players during regular season, which is 750. To take a simple random sample of \textit{n} = 120 of these baseball players and their salaries, we could number each player from 1 to 750. Then we could randomly select 120 numbers between 1 and 750 (without replacement) using a random number generator or random digit table. The players with the selected numbers would comprise our sample.

Two properties are always true in a simple random sample:
\begin{enumerate}
\item Each case in the population has an equal chance of being included in the sample.
\item Each \emph{group} of \textit{n} cases has an equal chance of making up the sample.
\end{enumerate}

The statistical methods in this book focus on data collected using simple random sampling. Note that Property 2 -- that each group of \textit{n} cases has an equal chance making up the sample -- is not true for the remaining four sampling techniques. As you read each one, consider why.

Though less common than simple random sampling, \termsub{systematic sampling}{sample!systematic sampling} is sometimes used when there exists a convenient list of all of the individuals of the population. Suppose we have a roster with the names of all the MLB players for a particular season. To take a systematic random sample, number them from 1 to 750. Select one random number between 1 and 750 and let that player be the first individual in the sample. Then, depending on the desired sample size, select every 10th number or 20th number, for example, to arrive at the sample.\footnote{If we want a sample of size \textit{n} = 150, it would make sense to select every 5th player because \mbox{$750/150 = 5$.} Suppose we randomly select the number 741. Then player 741, 746, 1, 6, 11, $\cdots$ , 731, and 736 would make up the sample.} If there are no patterns in the salaries based on the numbering then this could be a reasonable method.

\begin{examplewrap}
\begin{nexample}{A systematic sample is not the same as a simple random sample. Provide an example of a sample that can come from a simple random sample but not from a systematic random sample.}
Answers can vary. If we take a sample of size 3, then it is possible that we could sample players numbered 1, 2, and 3 in a simple random sample. Such a sample would be impossible from a systematic sample. Property~2 of simple random samples does not hold for other types of random samples.
\end{nexample}
\end{examplewrap}

Sometimes there is a variable that is known to be associated with the quantity we want to estimate. In this case, a stratified random sample might be selected. \termsub{Stratified sampling}{sample!stratified sampling} is a divide-and-conquer sampling strategy. The population is divided into groups called \term{strata}\index{sample!strata|textbf}. The strata are chosen so that similar cases are grouped together and a sampling method, usually simple random sampling, is employed to select a certain number or a certain proportion of the whole within each stratum. In the baseball salary example, the 30 teams could represent the strata; some teams have a lot more money (we're looking at you, Yankees).

\D{\newpage}

\begin{figure}[p]
\centering
\Figures[Three figures are shown, each positioned above the other. The first figure is a large rectangle which contains 6 larger circles that are labeled "Stratum 1" through "Stratum 6". In each of these circles are many points, and within each of the six circles, 3 points have been circled and are in a different color, illustrating a stratified random sample.  The second figure is a large rectangle containing 8 large circles with labels "Cluster 1" through "Cluster 8". All of these large circles contain points. However, three of the large circles (Cluster 3, Cluster 4, and Cluster 8) are colored differently than the other large circles and their contained points are also colored differently, illustrating a cluster random sample. ]{0.88}{samplingMethodsFigure}{stratified_cluster}
\caption{Examples of stratified and cluster sampling. In the top panel, stratified sampling was used: cases were grouped into strata, and then simple random sampling was employed within each stratum. In the bottom panel, cluster sampling was used, where data were binned into nine cluster and three clusters were randomly selected.}
\label{stratified_cluster}
\end{figure}

\begin{examplewrap}
\begin{nexample}{For this baseball example, briefly explain how to select a stratified random sample of size \textit{n} = 120. }
Each team can serve as a stratum, and we could take a simple random sample of 4 players from each of the 30 teams, yielding a sample of 120 players.
\end{nexample}
\end{examplewrap}

Stratified sampling is inherently different than simple random sampling. For example, the stratified sampling approach described would make it impossible for the entire Yankees team to be included in the sample.

\begin{examplewrap}
\begin{nexample}{Stratified sampling is especially useful when the cases in each stratum are very similar \emph{with respect to the outcome of interest}. Why is it good for cases within each stratum to be very similar?}
We should get a more stable estimate for the subpopulation in a stratum if the cases are very similar. These improved estimates for each subpopulation will help us build a reliable estimate for the full population. For example, in a simple random sample, it is possible that just by random chance we could end up with proportionally too many Yankees players in our sample, thus overestimating the true average salary of all MLB players. A stratified random sample can assure proportional representation from each team.
\end{nexample}
\end{examplewrap}

Next, let's consider a sampling technique that randomly selects groups of people. \termsub{Cluster sampling}{sample!cluster sampling} is much like simple random sampling, but instead of randomly selecting \emph{individuals}, we randomly select groups or \term{clusters}. Unlike stratified sampling, cluster sampling is most helpful when there is a lot of case-to-case variability within a cluster but the clusters themselves don't look very different from one another. That~is, we expect individual strata to be \term{homogeneous} (self-similar), while we expect individual clusters to be \term{heterogeneous} (diverse) with respect to the variable of interest.  

Sometimes cluster sampling can be a more economical random sampling technique than the alternatives. For example, if neighborhoods represented clusters, this sampling method works best when each neighborhood is very diverse. Because each neighborhood itself encompasses diversity, a cluster sample can reduce the time and cost associated with data collection, because the interviewer would need only go to some of the neighborhoods rather than to all parts of a city, in order to collect a useful sample.


\begin{examplewrap}
\begin{nexample}{Suppose we are interested in estimating the proportion of students at a certain school that have part-time jobs. It is believed that older students are more likely to work than younger students. What sampling method should be employed? Describe how to collect such a sample to get a sample size of 60.}
Because grade level affects the likelihood of having a part-time job, we should take a stratified random sample. To do this, we can take a simple random sample of 15 students from each grade. This will give us equal representation from each grade. Note: in a simple random sample, just by random chance we might get too many students who are older or younger, which could make the estimate too high or too low. Also, there are no well-defined clusters in this example. We~wouldn't want to use the grades as clusters and sample everyone from a couple of the grades. This would create too large a sample and would not give us the nice representation from each grade afforded by the stratified random sample.
\end{nexample}
\end{examplewrap}



\begin{onebox}{Advanced sampling techniques require advanced methods}
{The methods of inference covered in this book generally only apply to simple random samples. More advanced analysis techniques are required for systematic, stratified, and cluster random sampling.}
\end{onebox}


%%
\subsection*{Section summary}

\begin{itemize}


\item \termni{Generalizations} from a sample can be made to a population only if the sample is random.  Furthermore, the generalization can be made only to the population from which the sample was randomly selected, not to a larger or different population.  

\item Sampling \emph{without} replacement is a sampling strategy in which an observational
unit from a population can be selected only once. The observational unit is not
returned to the population before subsequent selections of observational units
are made, so there is no chance that the observational unit can be selected
again.

\item Sampling \emph{with} replacement is a sampling strategy in which an observational
unit from the population can be selected more than once. The observational
unit is returned to the population before subsequent selections of
observational units are made, so it is possible that the observational unit could
be selected again.

\item In a \termni{simple random sample} (SRS) of size $n$, every individual has the same chance of being selected and every sample of the size $n$ has the
same chance of being the sample selected. This method is the basis for many types of
sampling mechanisms. A common way to select a simple random sample is to number each individual of the population from 1 to N.  Using a random number generator, integers from 1 to N are randomly selected \emph{without} replacement and the corresponding individuals become part of the sample.

\item A \termni{stratified random sample} involves the division of all individuals in a population
into non-overlapping groups, called strata, that are similar in some what that might affect their responses.  Within each stratum a
simple random sample is selected, and the selected individuals are combined
to form one sample.

\item A \termni{cluster random sample} involves the division of a population into smaller
groups, called clusters. Ideally, each cluster mirrors the heterogeneity of the
population, with clusters similar to one another. A simple random sample of
clusters is selected from the population to form the sample of clusters. Data
are collected from all observational units in each of the selected clusters.  Cluster sampling can save time and money by collecting data from entire groups of individuals that may be close together.

\item A \termni{systematic random sample} is a method in which sample members from
a population are selected according to a random starting point and a fixed,
periodic interval between successive sampling units.  A systematic random sample is sometimes easier to conduct but one must beware of any patterns in the way the population is ordered.

\item Each random sampling method has different characteristics that make it
more appropriate for sampling populations depending on the question being
investigated. 

\item \termni{Bias} in a sampling method is a systematic error in the sampling procedure that
results in a statistic being consistently larger or consistently smaller than the
parameter the statistic is used to estimate.

\item \termni{Voluntary response bias} is a bias that may occur when a sample consists
entirely of volunteers.

\item \termni{Undercoverage bias} may occur when the sampling method fails to include part
of the population or a part of the population is less likely to be selected based
on the sampling method.

\item \termni{Non-response bias} may occur because of a failure to obtain responses from
some individuals chosen to be sampled. The respondents and nonrespondents
could differ significantly in ways that are important for the study.

\item \termni{Response bias} may occur when responses to a survey or measurements
of observational units tend to differ from the ``true" value in one direction.
Examples include questions that are confusing or leading (question wording
bias) or self-reported responses.

\item Nonrandom sampling methods (e.g., samples chosen by convenience or
voluntary response) introduce potential bias because they do not use random
chance to select the individuals.

\end{itemize}

%%%section exercises
{\input{ch_one_variable_data_collecting_data/TeX/sampling_methods_and_sources_of_bias.tex}}


%_____________________________________
\section[Experimental design]{Experimental design }
\label{experimentsSection}
\sectionintro{
\noindent%
Does the use of stents reduce the risk of stroke?  What are different ways to design an experiment to answer this question?
What are possible sources of bias, and how can we try to minimize them?  If we do not incorporate principles of good experimental design, we will not be able to draw a causal conclusion about the effectiveness of stents.  This is why it is crucial to start with a well-designed experiment.


\subsection*{Learning objectives}
\begin{enumerate}
\setlength{\itemsep}{0mm}
\item Identify elements of a well-designed experiment.

\item Identify experimental designs.
 
\item Justify the appropriateness of a particular experimental design.

\item Justify the appropriateness of conclusions based on a well-designed experiment.
 
\end{enumerate}\D{\vspace{-6mm}}
}



%%
\subsection{Case study: using stents to prevent strokes}
\label{basicExampleOfStentsAndStrokes}
Here, we introduce a classic challenge in statistics: evaluating the effectiveness of a medical treatment. Terms introduced here will be more explicitly defined later in the section.  The goal for now is simply to get a sense of the role statistics can play in practice.

In this section we will consider an experiment that studies effectiveness of stents in treating patients at risk of stroke.  Stents are devices put inside blood vessels that assist in patient recovery after cardiac events and reduce the risk of an additional heart attack or death. Many doctors have hoped that there would be similar benefits for patients at risk of stroke. We start by writing the principal question the researchers hope to answer:
\begin{quote}
Does the use of stents reduce the risk of stroke?
\end{quote}

\noindent The researchers who asked this question collected data on 451 at-risk patients. Each volunteer patient was randomly assigned to one of two groups:
\begin{itemize}
\item[]\termsub{Treatment group}{treatment group}. Patients in the treatment group received a stent and medical management. The medical management included medications, management of risk factors, and help in lifestyle modification.
\item[]\termsub{Control group}{control group}. Patients in the control group received the same medical management as the treatment group, but they did not receive stents.
\end{itemize}
Researchers randomly assigned 224 patients to the treatment group and 227 to the control group. In this study, the control group provides a reference point against which we can measure the medical impact of stents in the treatment group.

Researchers studied the effect of stents at two time points: 30~days after enrollment and 365~days after enrollment. The results of 5 patients are summarized in Figure~\ref{stentStudyResultsDF}. Patient outcomes are recorded as ``stroke'' or ``no event'', representing whether or not the patient had a stroke at the end of a time period.

\begin{figure}[h]
\centering
\begin{tabular}{l ccc}
\hline
Patient	&	group	&	0-30 days 	&	0-365 days \\
\hline
1		&	treatment &	no event &	no event \\
2		&	treatment &	stroke & stroke \\
3		&	treatment &	no event & no event \\
$\vdots$	&	$\vdots$	  &	$\vdots$ \\
450	&	control &	no event &	no event \\
451	&	control &	no event &	no event \\
\hline
\end{tabular}
\caption{Results for five patients from the stent study.}
\label{stentStudyResultsDF}
% trmt <- c(rep('trmt', 224), rep('control', 227)); outcome30 <- c(rep(c('event', 'no_event'), c(33, 191)), rep(c('event', 'no_event'), c(13, 214))); outcome365 <- c(rep(c('event', 'no_event'), c(33, 191)), rep(c('event', 'no_event'), c(13, 214)))
\end{figure}

Considering data from each patient individually would be a long, cumbersome path towards answering the original research question. Instead, performing a statistical data analysis allows us to consider all of the data at once. Figure~\ref{stentStudyResults} summarizes the raw data in a more helpful way. In this table, we can quickly see what happened over the entire study. For instance, to identify the number of patients in the treatment group who had a stroke within 30 days, we look on the left-side of the table at the intersection of the treatment and stroke: 33.

\begin{figure}[h]
\centering
\begin{tabular}{l cc c cc}
& \multicolumn{2}{c}{0-30 days} &\hspace{5mm}\ & \multicolumn{2}{c}{0-365 days} \\
  \cline{2-3} \cline{5-6}
	& 	stroke 	& no event && 	stroke 	& no event \\
  \hline
treatment 	& 33		& 191	&&	45 	& 179 \\
control 		& 13		& 214	&& 	28	& 199 \\
  \hline
Total				& 46		& 405	&&	73	& 378 \\
  \hline
\end{tabular}
\caption{Descriptive statistics for the stent study.}
\label{stentStudyResults}
\end{figure}

\begin{exercisewrap}
\begin{nexercise}
What proportion of the patients in the treatment group had no stroke within the first 30 days of the study? (Please note: answers to all Guided Practice exercises are provided using footnotes.)\footnotemark
\end{nexercise}
\end{exercisewrap}
\footnotetext{There were 191 patients in the treatment group that had no stroke in the first 30 days. There were 33 + 191 = 224 total patients in the treatment group, so the proportion is $191 / 224 = 0.85$.}

We can compute summary statistics from the table. A \term{summary statistic} is a single number summarizing a large amount of data.\footnote{Formally, a summary statistic is a value computed from the data. Some summary statistics are more useful than others.} For instance, the primary results of the study after 1~year could be described by two summary statistics: the proportion of people who had a stroke in the treatment and control groups.
\begin{itemize}
\setlength{\itemsep}{0mm}
\item[] Proportion who had a stroke in the treatment (stent) group: $45/224 = 0.20 = 20\%$.
\item[] Proportion who had a stroke in the control group: $28/227 = 0.12 = 12\%$.
\end{itemize}
These two summary statistics are useful in looking for differences in the groups, and we are in for a surprise: an additional 8\% of patients in the treatment group had a stroke! This is important for two reasons. First, it is contrary to what doctors expected, which was that stents would \emph{reduce} the rate of strokes. Second, it leads to a statistical question: do the data show a ``real'' difference between the groups?

This second question is subtle. Suppose you flip a coin 100 times. While the chance a coin lands heads in any given coin flip is 50\%, we probably won't observe exactly 50 heads. This type of fluctuation is part of almost any type of data generating process. It is possible that the 8\% difference in the stent study is due to this natural variation. However, the larger the difference we observe (for a particular sample size), the less believable it is that the difference is due to chance. So what we are really asking is whether the difference is \term{statistically significant}, that is, whether the difference so large that we should reject the notion that it was due to chance.  To answer this question, we will need to carry out an inference procedure called a hypothesis test, which is introduced in Chapter~\ref{ch_inference_for_props}.

While we don't yet have the statistical tools to fully address this question on our own, we can comprehend the conclusions of the published analysis: there was compelling evidence of harm by stents in this study of stroke patients.

\textbf{Be careful:} do not generalize the results of this study to all patients and all stents. This study looked at patients with very specific characteristics who volunteered to be a part of this study and who may not be representative of all stroke patients. In addition, there are many types of stents and this study only considered the self-expanding Wingspan stent (Boston Scientific). However, this study does leave us with an important lesson: we should keep our eyes open for surprises.

\index{data!stroke|)}


\D{\newpage}

%%
\subsection{Reducing bias in human experiments}
\label{biasInHumanExperiments}

Just as random selection is essential in sampling in order to avoid selection bias, \termni{random assignment} is essential in the context of experiments to determine which subjects will receive which treatments. If the researcher chooses which patients are in the treatment and control groups, she may unintentionally place sicker patients in the treatment group, biasing the experiment against the treatment.

Randomized experiments are essential for investigating cause and effect relationships, but they do not ensure an unbiased perspective in all cases. Human studies are perfect examples where bias can unintentionally arise. Here we reconsider a study where a new drug was used to treat heart attack patients. In particular, researchers wanted to know if the drug reduced deaths in patients.

These researchers designed a randomized experiment because they wanted to draw causal conclusions about the drug's effect. Study volunteers\footnote{Human subjects are often called \term{patients}, \term{volunteers}, or \term{study participants}.} were randomly placed into two study groups. One group, the \term{treatment group}, received the drug. The other group, called the \term{control group}, did not receive any drug treatment. In an experiment, the explanatory variable is also called a \term{factor}. Here the factor is receiving the drug treatment. It has two \term{levels}: yes and no, thus it is categorical. The response variable is whether or not patients died within the time frame of the study. It is also categorical.

Put yourself in the place of a person in the study. If you are in the treatment group, you are given a fancy new drug that you anticipate will help you. On the other hand, a person in the other group doesn't receive the drug and sits idly, hoping her participation doesn't increase her risk of death. These perspectives suggest there are actually two effects: the one of interest is the effectiveness of the drug, and the second is an emotional effect that is difficult to quantify.

Researchers aren't usually interested in the emotional effect, which might bias the study. To circumvent this problem, researchers do not want patients to know which group they are in. When researchers keep the patients uninformed about their treatment, the study is said to be \term{blind} or \term{single-blind}. But there is one problem: if a patient doesn't receive a treatment, she will know she is in the control group. The solution to this problem is to give fake treatments to patients in the control group. A fake treatment is called a \term{placebo}, and an effective placebo is the key to making a study truly blind. A classic example of a placebo is a sugar pill that is made to look like the actual treatment pill. Often times, a placebo results in a slight but real improvement in patients. This effect has been dubbed the \term{placebo~effect}.

The patients are not the only ones who should be blinded: doctors and researchers can accidentally bias a study. When a doctor knows a patient has been given the real treatment, she might inadvertently give that patient more attention or care than a patient that she knows is on the placebo. To guard against this bias, which again has been found to have a measurable effect in some instances, most modern studies employ a \term{double-blind} setup where researchers who interact with subjects and are responsible for measuring the response variable are, just like the subjects, unaware of who is or is not receiving the treatment.\footnote{There are always some researchers involved in the study who do know which patients are receiving which treatment. However, they do not interact with the study's patients and do not tell the blinded health care professionals who is receiving which treatment.}


\begin{exercisewrap}
\begin{nexercise}
Look back to the study in subsection~\ref{basicExampleOfStentsAndStrokes} where researchers were testing whether stents were effective at reducing strokes in at-risk patients. Is this an experiment? Was the study blinded? Was it double-blinded?\footnotemark
\end{nexercise}
\end{exercisewrap}
\footnotetext{The researchers assigned the patients into their treatment groups, so this study was an experiment. However, the patients could distinguish what treatment they received, so this study was not blind. The study could not be double-blind because it was not blind.}


\D{\newpage}

%%
\subsection{Elements of a well-designed experiment}
\label{experimentalDesignPrinciples}

\noindent The goal of an experiment is to be able to draw a \emph{causal} conclusion about the effect of a treatment.  Well-designed experiments are built on four main principles.  
\begin{description}
\setlength{\itemsep}{0mm}
\item[Comparison of Treatment Groups.] In order to determine if a treatment is effective, there should be at least two treatment groups, one of which could be a control group.  At the end of an experiment, the groups will be \emph{compared} to determine if one treatment is, overall, more effective than another treatment or a placebo.  Where possible the experiment should be \termni{double-blind}, with neither the subjects nor the researchers knowing who is receiving which treatment.

\item[Random Assignment.] Subjects/experimental units should be randomly assigned to treatment groups (or treatments randomly assigned to subjects/experimental units).  The purpose of the random assignment is to make the treatment groups as similar as possible with respect to \term{extraneous variables} -- variables that could be associated with or impact the response variable.  If random assignment is successful, the distributions of extraneous variables will be approximately the same for the treatments groups.  For example, some subjects may be more susceptible to a disease than others due to dietary or genetic factors. Randomizing subjects into treatment groups helps \emph{even out} the effects of such variables, and it also prevents accidental bias that result from choosing which treatment each subject/experimental unit receives.

\item[\termsub{Replication}{replication}.] Replication within an experiment means multiple subjects/experimental units are assigned to each treatment.  The more cases researchers observe, the more accurately they can estimate the effect of the explanatory variable on the response. Also, in an experiment with six subjects, even if there is randomization, it is quite possible for the three healthiest people to be in the same treatment group. In a randomized experiment with 100 people, it is virtually impossible for the healthiest 50 people to end up in the same treatment group. Thus replication makes it more likely that the randomization will be successful in evening out the effects of extraneous variables that are associated with or impact the response variable.

\item[\termsub{Direct Control}{direct control}.] Direct control in an experiment means keeping the settings of certain potential extraneous sources of variation in the response variable the same from
experimental unit to experimental unit. For example, when patients take a drug in pill form, some patients take the pill with only a sip of water while others may have it with an entire glass of water. To control for the effect of water consumption, a doctor may ask all patients to drink a 12 ounce glass of water with the pill.  A researcher can directly control the appearance of the treatment, the time of day it is taken, etc.  She cannot directly control variables such as gender or age. To control for these types of variables, she might consider blocking, which is described in Section~\ref{CompletelyRandomizedBlockedAndMatchedPairsDesign}.

\end{description}

Sometimes, at the end of an experiment, a researcher may find that even though these four design elements were used, an extraneous variable, such as whether or not a subject smokes, was still unevenly distributed between the treatment groups.  In this case, this extraneous variable of smoking is a \termni{confounding variable}, as it is associated with the response variable \emph{and} is more present in one treatment group than another (making it associated with the explanatory variable as well).  This is why researchers often record information on extraneous variables -- it allows them to determine if the randomization was effective at making the treatment groups similar with respect to those extraneous variables.   In the next section we will consider three types of experimental design.


%%
\subsection{Completely randomized, blocked, and matched pairs design}
\label{CompletelyRandomizedBlockedAndMatchedPairsDesign}

\index{completely randomized experiment|(}
\index{blocked experiment|(}
\index{matched pairs|(}

A \term{completely randomized experiment} is one in which the subjects or experimental units are randomly assigned to each treatment group in the experiment. Suppose we have two treatments, one of which may be a placebo, and 300 subjects. To carry out a completely randomized design, we could assign each subject a unique integer from 1 to 300.  Then we could use a random number generator to randomly generate 150 integers from 1 to 300 \emph{without replacement}.  The subjects assigned those integers would go in the first treatment group.  The remaining subjects would go in the second treatment group.  Note that this method of randomly assigning subjects to treatments is not equivalent to taking a simple random sample. Here we are not sampling a subset of a population; we are randomly \emph{splitting} subjects into groups.

% ZZQ - The following two paragraphs have an awkward transition.

\D{\newpage}

Researchers sometimes know or suspect that another variable, other than the treatment, influences the response. Under these circumstances, they may carry out a \term{blocked experiment}. In this design, they first group individuals into \term{blocks} based on the identified extraneous variable and then randomize subjects within each block to the treatment groups. This strategy is referred to as \term{blocking}. For instance, if we are looking at the effect of a drug on heart attacks, we might first split patients in the study into low-risk and high-risk blocks. Then we can randomly assign half the patients from each block to the control group and the other half to the treatment group, as shown in Figure~\ref{figureShowingBlocking}. At the end of the experiment, we would incorporate this blocking into the analysis. By blocking by risk of patient, we control for this possible confounding factor. Additionally, by randomizing subjects to treatments within each block, we attempt to even out the effect of variables that we cannot block or directly control.

\begin{figure}
\centering
\Figure [There are three main stages shown in this figure, from top to bottom. The upper stage shows the numbering of patients as a rectangle containing 54 dots in a grid that are labeled with numbers 1 through 54. The dots are one of two colors: blue (high risk) and red (low risk). The second stage shows the two colored dots broken into two blocks. On the left are the low-risk patients (red) and on the right are the high-risk patients (blue). Going into the bottom third stage, are two boxes labeled "control" and "treatment", where half of the low-risk (red) and half of the blue (high risk) points have been randomly placed into each of these two experiment groups.]{0.78}{figureShowingBlocking}
\caption{Blocking using a variable depicting patient risk. Patients are first divided into low-risk and high-risk blocks, then each block is evenly separated into the treatment groups using randomization. This strategy ensures an equal representation of patients in each treatment group from both the low-risk and high-risk categories.}
\label{figureShowingBlocking}
\end{figure}

\begin{examplewrap}
\begin{nexample}{An experiment will be conducted to compare the effectiveness of two methods for quitting smoking. Identify a variable that the researcher might wish to use for blocking and describe how she would carry out a blocked experiment.}
The researcher should choose the variable that is most likely to influence the response variable - whether or not a smoker will quit. A reasonable variable, therefore, would be the number of years that the smoker has been smoking. The subjects could be separated into three blocks based on number of years of smoking and each block randomly divided into the two treatment groups.
\end{nexample}
\end{examplewrap}

Even in a blocked experiment with randomization, other variables that affect the response can be distributed unevenly among the treatment groups, thus biasing the experiment in one direction. A third type of design, known as \term{matched pairs} addresses this problem. In a matched pairs experiment, pairs of people are matched on as many variables as possible, so that the comparison happens between very similar cases. This is actually a special type of blocked experiment, where the blocks are of size two.

An alternate form of matched pairs involves each subject receiving \emph{both} treatments. Randomization can be incorporated by randomly selecting half the subjects to receive treatment 1 first, followed by treatment 2, while the other half receives treatment 2 first, followed by treatment.

\begin{exercisewrap}
\begin{nexercise}
How and why should randomization be incorporated into a matched pairs design?\footnotemark
\end{nexercise}
\end{exercisewrap}
\footnotetext{Assume that all subjects received treatment 1 first, followed by treatment 2. If the variable being measured happens to increase naturally over the course of time, it would appear as though treatment 2 had a greater effect than it really did.} 

%\Add{This type of matched pairs design is optimal, because it allows us to compare each subject to herself, rather than comparing a group of subjects to a separate group of subjects. By doing this, we control for the inherent variability in response from person to person.} % This is not necessarily true: treatments can interact in unexpected ways, even if given over long periods of time. Even in the lotion example, handedness might be more important in some situations, e.g. if looking at how much lotion rubs off. Careful matched individuals might be a better design in that situation.

\begin{exercisewrap}
\begin{nexercise}
Matched pairs sometimes involves each subject receiving both treatments at the same time. For example, if a hand lotion was being tested, half of the subjects could be randomly assigned to put Lotion A on the left hand and Lotion B on the right hand, while the other half of the subjects would put Lotion B on the left hand and Lotion A on the right hand. Why would this be a better design than a completely randomized experiment in which half of the subjects put Lotion A on both hands and the other half put Lotion B on both hands?\footnotemark\end{nexercise}
\end{exercisewrap}
\footnotetext{The dryness of people's skins varies from person to person, but probably less so from one person's right hand to left hand. With the matched pairs design, we are able control for this variability by comparing each person's right hand to her left hand, rather than comparing some people's hands to other people's hands (as you would in a completely randomized experiment).} % In a completely randomized experiment, it is possible that one group, just by chance, could have more people with especially dry skin, thus biasing the results against the lotion that they used.} % Cutting for brevity.

Because it is essential to identify the type of data collection method used when choosing an appropriate inference procedure, we will revisit sampling techniques and experiment design in the subsequent chapters on inference.

\index{matched pairs|)}
\index{blocked experiment|)}
\index{completely randomized experiment|)}


\D{\newpage}

%%
\subsection{Testing more than one variable at a time}

Some experiments study more than one factor (explanatory variable) at a time, and each of these factors may have two or more levels (possible values). For example, suppose a researcher plans to investigate how the type and volume of music affect a person's performance on a particular video game. Because these two factors, \var{type} and \var{volume}, could interact in interesting ways, we do not want to test one factor at a time. %
%Instead, we want to do a \term{factorial experiment} in which 
Instead, we want to do an experiment in which
we test all \emph{combinations} of the factors. Let's say that \var{volume} has two levels (soft and loud) and that \var{type} has three levels (dance, classical, and punk). Then, we would want to have experiment groups for each of the six (2 x 3 = 6) combinations: soft dance, soft classical, soft punk, loud dance, loud classical, loud punk. Each combination is a \term{treatment}. Therefore, this experiment will have 2 factors and 6~treatments. To replicate each treatment 10 times, one would need to play the game 60~times.

\begin{exercisewrap}
\begin{nexercise}A researcher wants to compare the effectiveness of four different drugs. She also wants to test each of the drugs at two doses: low and high. Describe the factors, levels, and treatments of this experiment.\footnotemark
\end{nexercise}
\end{exercisewrap}
\footnotetext{There are two factors: type of drug, which has four levels, and dose, which has 2 levels. There will be 4 x 2 = 8 treatments: drug 1 at low dose, drug 1 at high dose, drug 2 at low dose, and so on.}

As the number of factors and levels increases, the number of treatments become large and the analysis of the resulting data becomes more complex, requiring the use of advanced statistical methods. We will investigate only one factor at a time in this book.

\subsection{Drawing conclusions based on experiments}

The goal in an experiment is for the treatment groups to be as similar as possible \emph{except for the treatment}, so that at the end of the experiment any difference in response between the groups can be attributed to the treatment and not to some confounding variable.  Using random assignment of subjects/experimental units to treatment groups allows for a cause-and-effect conclusion between the explanatory and response variables because the potential for confounding variables is reduced.  

To be able to generalize this cause-and-effect relationship to a larger population, a random sample is needed.  However, for ethical and practical reasons, subjects are rarely randomly sampled from a larger population; instead, most subjects are volunteers.  For this reason, the precise population for which the conclusion applies may be unclear. For example, if an experiment to determine the most effective means to encourage individuals to vote is carried out only on college students, we may not be able to generalize the conclusions of the experiment to all adults in the population.  While technically a generalization to a larger population is not valid without a random sample, in practice we say that the results of an experiment can be generalized to the population of subjects/experimental units similar to those that participated in the experiment.  You will see this language later in the book when we investigate hypothesis testing for completely randomized experiments and matched pairs experiments.   

\D{\newpage}

%%
\subsection*{Section summary}

\begin{itemize}
\item In an \termni{experiment}, researchers impose a \termni{treatment} to test its effects, with the goal being to draw a causal conclusion between the type of treatment (explanatory variable) and the response variable.  In order for observed differences in the response to be attributed to the treatment and not to some other factor, it is important to make the treatment groups and the conditions for the treatment groups as similar as possible.

\item A well-designed experiment should include comparison of at least two treatment groups, random assignment, replication, and direct control.  

\item A \termni{control group} is a collection of experimental units that are created for
comparison. A control group may be given a treatment different from the
treatment of interest to determine if the treatment of interest has an effect
(e.g., a treatment with an inactive substance, a placebo, may be given).

\item The \termni{placebo effect} is the difference between the average response to a
placebo and the average response to no treatment.

\item In a \termni{single-blind}, also called single-masked, experiment, participants do not
know which treatment they are receiving, but members of the research team
who interact with them know which treatment each participant is receiving, or
vice versa

\item In a \termni{double-blind}, also called double-masked, experiment, neither the
participants nor the members of the research team who interact with them
know which treatment each participant is receiving.

\item An extraneous source of variation, also referred to as an \termni{extraneous variable},
is a variable that is known (or believed) to affect the response but is not an
explanatory variable being studied.

\item The purpose of \termni{random assignment} is to create treatment groups that are as
similar as possible with respect to extraneous sources of variation. If random
assignment is successful, the respective distributions of each extraneous
variable will be approximately the same for all the treatment groups, thus \emph{evening out} the effects of extraneous variables.

\item A \termni{confounding variable} in an experiment is a variable that is distributed
differently among treatment groups \emph{and} affects the response variable.

\item \termni{Replication}, or imposing the treatments on multiple subjects or experimental units, provides more data for comparison and decreases the likelihood that the treatment groups differ on some characteristic due to chance alone (i.e. in spite of the randomization).  

\item \termni{Direct control} in an experiment means keeping variables that are within a researchers power to control (e.g. testing conditions) the \emph{same} between treatment groups and from experimental unit to experimental unit.  Direct control in an experiment helps to control for potential extraneous variables and sources of variation in the response variable.

\item In a \termni{completely randomized design}, subjects or experimental units are randomly assigned to different treatment groups.  To carry out the random assignment, first number the units with unique integers from 1 to N.  Then, use a random number generator to randomly choose integers from 1 to N without replacement and assign the subjects/units corresponding to those integers to a treatment group.  Repeat as needed.  Do this in such a way that the treatment group sizes are balanced, unless there exists a good reason to make one treatment group larger than another.

\D{\newpage}

\item In a \termni {blocked design}, experimental units are first separated by a blocking variable thought to be associated with the response variable.  Units are separated into groups or \termni{blocks}, with each block being homogeneous or alike with respect to the blocking variable (e.g. block subjects by smoking status and create a smoking block and a nonsmoking block, when smoking is thought to be associated with the response variable being measured).  Within \textit{each} block, subjects are randomly assigned to the treatment groups, allowing the researcher to compare like to like within each block.   Blocking allows for more precise comparisons of the response across the treatments, because within each block the treatments can be compared without having to worry about variation in the response being due to differences in the blocking variable.

\item A \termni{matched pairs design} uses a randomized block design where each block consists of a pair of experimental units that are as similar as possible.
Each pair receives both treatments by randomly assigning one treatment to one member of the
pair and the other treatment to the second member of the pair. Alternatively, instead of each pair receiving one treatment, a matched pairs experiment may involve each experimental unit getting \emph{both} treatments, with the order of the
treatments being randomized.  A matched pairs design allows for the best comparison of like to like.  

\item A completely randomized, blocked, or matched pairs design may be more appropriate depending on the goals of the experiment, the characteristics of the subjects or experimental units, and the variables involved.

\item Using random assignment of treatments to experimental units allows for
cause-and-effect conclusions between the explanatory and the response
variables because the potential for confounding variables is reduced.

\item Depending on the experimental unit, it may be unethical or difficult to randomly
select subjects or experimental units to participate in an experiment. In that case, the
study’s experimental units are obtained from volunteers and will represent the
population of experimental units similar to those who participated in the study.

\end{itemize}


%%%section exercises
{\input{ch_one_variable_data_collecting_data/TeX/experimental_design.tex}}

%__________________________________
\reviewchapterheader{}


\noindent A raw data matrix/table may have thousands of rows. The data need to be summarized in order to make sense of all the information. In this chapter, we looked at ways to summarize data graphically, numerically, and verbally.  We also investigated various ways that researchers collect data. The key concepts are the difference between a sample and an experiment, the role that randomization plays in each, and the types of conclusions that can be drawn.
\\
\\
\textbf{Categorical data}
\begin{itemize}


\item A single \textbf{categorical variable} is summarized with \textbf{counts} or \textbf{proportions} in a \textbf{frequency table} or \textbf{relative frequency table}.  A \textbf{bar~chart} is used to show the frequency or relative frequency of the categories that the variable takes on.


\end{itemize}
\textbf{Numerical data}
\begin{itemize}
\item When looking at a single \textbf{numerical variable}, we try to understand the \textbf{distribution} of the variable.  The distribution of a variable can be represented with a frequency or relative frequency table and with a graph, such as a \textbf{stem-and-leaf plot} or \textbf{dot plot} for small data sets, or a \textbf {histogram} for larger data sets.  If only a summary is desired, a \textbf{box plot} may be used to graph the \textbf{five-number summary}.
\item The \textbf{distribution} of a variable can be described and summarized with \textbf{center} (mean or median), \textbf{spread} (SD or IQR), and \textbf{shape} (right skewed, left skewed, approximately symmetric).  It is also helpful to note any unusual features such as outliers,
gaps, or clusters.

\item \textbf{Z-scores} and \textbf{percentiles} are useful for identifying a data point's relative position within a data set.

\item \textbf{Outliers} are values that appear extreme relative to the rest of the data.  Investigating outliers can provide insight into properties of the data or may reveal data collection/entry errors.

\item When \textbf{comparing the distribution} of two numerical variables, use two dot plots, two histograms, a back-to-back stem-and-leaf, or parallel box plots.
\end{itemize}


\noindent \textbf{Collecting data} 
\begin{itemize}
\item Researchers take a \termni{random sample} in order to draw an \termni{inference} to the larger population from which they sampled.  When examining observational data, even if the individuals were randomly sampled, a correlation does not imply a causal link.  

\item In an \termni{experiment}, researchers impose a treatment and use \termni{random assignment} in order to make a \textbf{comparison} and draw \termni{causal conclusions} about the effects of the treatment.  While often implied, inferences to a larger population may not be valid if the subjects were not also \textit{randomly sampled} from that population.

\item \textbf{\termsub{Stratifying}{stratifying} vs \termsub{Blocking}{blocking}}.  Stratifying is used when sampling, where the purpose is to \emph{sample} a subgroup from each stratum in order to arrive at a better \textit{estimate} for the parameter of interest.  Blocking is used in an experiment to \emph{separate} subjects into blocks and then \emph{compare} responses within those blocks.  All subjects in a block are used in the experiment, not just a sample of them.


\item Always be mindful of possible \termni{confounding factors} when interpreting the results of observation studies.

\end{itemize} 

\noindent It is the role of the researcher or data scientist to ask questions, to identify patterns and departure from patterns, and to make sense of this in the context of the data.  Strong writing and presentation skills are critical for being able to communicate the methods and results to a wider audience.  

